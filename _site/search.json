[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "library(Matching)\n\n#&gt; Loading required package: MASS\n\n\n#&gt; ## \n#&gt; ##  Matching (Version 4.10-14, Build Date: 2023-09-13)\n#&gt; ##  See https://www.jsekhon.com for additional documentation.\n#&gt; ##  Please cite software as:\n#&gt; ##   Jasjeet S. Sekhon. 2011. ``Multivariate and Propensity Score Matching\n#&gt; ##   Software with Automated Balance Optimization: The Matching package for R.''\n#&gt; ##   Journal of Statistical Software, 42(7): 1-52. \n#&gt; ##\n\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2\n\n\n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks ggdag::filter(), stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ✖ dplyr::select() masks MASS::select()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(MatchIt)\n\n\ndata &lt;- readRDS(\"data/rand_enc.rds\")\ndata"
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)\nlibrary(Matching)\nlibrary(MatchIt)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(GGally)\nlibrary(dplyr)"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(tidyverse)\n\n\n\n\n# Define the DAG for the parking spot example\nparking_dag &lt;- dagify(\n  sales ~ parking_spots,\n  sales ~ location,\n  parking_spots ~ location,\n  coords = list(x = c(sales = 1, parking_spots = 3, location = 2),\n                y = c(sales = 0, parking_spots = 0, location = 1))\n)\n\n\n# Plot the DAG with default ggplot2 theme\nggdag(parking_dag, text = FALSE) +\n  geom_dag_point() +\n  geom_dag_text(color = \"red\") +\n  geom_dag_edges(edge_color = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Load the data file customer_sat.rds\ndata &lt;- readRDS(\"data/customer_sat.rds\")\n\n# 1. Regress satisfaction on follow_ups\nmodel_1 &lt;- lm(satisfaction ~ follow_ups, data = data)\n\nsummary(model_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\n\n\n\n\n# 2. Regress satisfaction on follow_ups and account for subscription\nmodel_2 &lt;- lm(satisfaction ~ follow_ups + subscription, data = data)\n\nsummary(model_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\n\n\nAccording to the first regression, there is a negative relationship between follow_ups and satisfaction, indicating that as the number of follow-up calls increases, so does satisfaction.\nThe second regression, which takes into account both follow_ups and subscription levels, reveals that, while follow_ups continue to have an impact on satisfaction, subscription levels (Premium and Premium+) also have a significant impact. Furthermore, the subscription level coefficients imply that higher subscription tiers (Premium and Premium+) have a positive impact on satisfaction when compared to some assumed base level (possibly Starter, which is not explicitly included as a variable).\n\n\n\n\n# Create a DAG for the relationship between follow-up calls, satisfaction, and subscription\nsaas_dag &lt;- dagify(\n  satisfaction ~ follow_ups,\n  satisfaction ~ subscription,\n  follow_ups ~ subscription,\n  coords = list(x = c(satisfaction = 1, follow_ups = 3, subscription = 2),\n                y = c(satisfaction = 0, follow_ups = 0, subscription = 1))\n)\n\n# Plot the DAG \nggdag(saas_dag, text = FALSE) +\n  geom_dag_point() +\n  geom_dag_text(color = \"red\") +\n  geom_dag_edges(edge_color = \"black\")"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "#load libraries\nlibrary(dplyr)\nlibrary(ggplot2)\n\n#Read the data and check dimensions\ndata &lt;- readRDS(\"data/car_prices.rds\")\ndim(data) # Check dimensions - rows and columns\n\n#&gt; [1] 181  22\n\n\n\n\n\n\n#Get a more detailed look at the data\nstr(data) # Display structure and data types\n\n#&gt; tibble [181 × 22] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ aspiration      : chr [1:181] \"std\" \"std\" \"std\" \"std\" ...\n#&gt;  $ doornumber      : chr [1:181] \"two\" \"two\" \"two\" \"four\" ...\n#&gt;  $ carbody         : chr [1:181] \"convertible\" \"convertible\" \"hatchback\" \"sedan\" ...\n#&gt;  $ drivewheel      : chr [1:181] \"rwd\" \"rwd\" \"rwd\" \"fwd\" ...\n#&gt;  $ enginelocation  : chr [1:181] \"front\" \"front\" \"front\" \"front\" ...\n#&gt;  $ wheelbase       : num [1:181] 88.6 88.6 94.5 99.8 99.4 ...\n#&gt;  $ carlength       : num [1:181] 169 169 171 177 177 ...\n#&gt;  $ carwidth        : num [1:181] 64.1 64.1 65.5 66.2 66.4 66.3 71.4 71.4 71.4 67.9 ...\n#&gt;  $ carheight       : num [1:181] 48.8 48.8 52.4 54.3 54.3 53.1 55.7 55.7 55.9 52 ...\n#&gt;  $ curbweight      : num [1:181] 2548 2548 2823 2337 2824 ...\n#&gt;  $ enginetype      : chr [1:181] \"dohc\" \"dohc\" \"ohcv\" \"ohc\" ...\n#&gt;  $ cylindernumber  : chr [1:181] \"four\" \"four\" \"six\" \"four\" ...\n#&gt;  $ enginesize      : num [1:181] 130 130 152 109 136 136 136 136 131 131 ...\n#&gt;  $ fuelsystem      : chr [1:181] \"mpfi\" \"mpfi\" \"mpfi\" \"mpfi\" ...\n#&gt;  $ boreratio       : num [1:181] 3.47 3.47 2.68 3.19 3.19 3.19 3.19 3.19 3.13 3.13 ...\n#&gt;  $ stroke          : num [1:181] 2.68 2.68 3.47 3.4 3.4 3.4 3.4 3.4 3.4 3.4 ...\n#&gt;  $ compressionratio: num [1:181] 9 9 9 10 8 8.5 8.5 8.5 8.3 7 ...\n#&gt;  $ horsepower      : num [1:181] 111 111 154 102 115 110 110 110 140 160 ...\n#&gt;  $ peakrpm         : num [1:181] 5000 5000 5000 5500 5500 5500 5500 5500 5500 5500 ...\n#&gt;  $ citympg         : num [1:181] 21 21 19 24 18 19 19 19 17 16 ...\n#&gt;  $ highwaympg      : num [1:181] 27 27 26 30 22 25 25 25 20 22 ...\n#&gt;  $ price           : num [1:181] 13495 16500 16500 13950 17450 ...\n\nglimpse(data)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\n\nFor strings  Character and for numbers  Double data types are observed\n\n\n\n\n#We Run a linear regression \nmodel &lt;- lm(price ~ ., data = data[, -20])\nsummary(model) # Check regression summary for coefficients, p-values, etc.\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = data[, -20])\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -5715.4 -1192.9   -27.4   828.6  8978.4 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -3.553e+04  1.536e+04  -2.313 0.022149 *  \n#&gt; aspirationturbo       1.797e+03  1.035e+03   1.736 0.084626 .  \n#&gt; doornumbertwo         2.242e+02  5.695e+02   0.394 0.694471    \n#&gt; carbodyhardtop       -3.676e+03  1.421e+03  -2.587 0.010669 *  \n#&gt; carbodyhatchback     -3.338e+03  1.235e+03  -2.702 0.007710 ** \n#&gt; carbodysedan         -2.296e+03  1.353e+03  -1.697 0.091801 .  \n#&gt; carbodywagon         -3.444e+03  1.486e+03  -2.317 0.021887 *  \n#&gt; drivewheelfwd        -4.314e+02  1.066e+03  -0.405 0.686166    \n#&gt; drivewheelrwd        -1.558e+01  1.265e+03  -0.012 0.990190    \n#&gt; enginelocationrear    6.858e+03  2.535e+03   2.705 0.007659 ** \n#&gt; wheelbase            -3.977e+01  9.086e+01  -0.438 0.662247    \n#&gt; carlength            -2.392e+01  5.042e+01  -0.474 0.635924    \n#&gt; carwidth              7.244e+02  2.436e+02   2.974 0.003444 ** \n#&gt; carheight             1.163e+02  1.337e+02   0.870 0.385828    \n#&gt; curbweight            2.621e+00  1.776e+00   1.475 0.142293    \n#&gt; enginetypedohcv      -7.913e+03  4.595e+03  -1.722 0.087168 .  \n#&gt; enginetypel           1.101e+03  1.768e+03   0.623 0.534398    \n#&gt; enginetypeohc         3.440e+03  9.141e+02   3.764 0.000243 ***\n#&gt; enginetypeohcf        8.991e+02  1.616e+03   0.556 0.578818    \n#&gt; enginetypeohcv       -6.170e+03  1.230e+03  -5.018 1.52e-06 ***\n#&gt; cylindernumberfive   -1.154e+04  2.992e+03  -3.856 0.000173 ***\n#&gt; cylindernumberfour   -1.135e+04  3.147e+03  -3.605 0.000429 ***\n#&gt; cylindernumbersix    -6.918e+03  2.200e+03  -3.144 0.002023 ** \n#&gt; cylindernumberthree  -4.249e+03  4.676e+03  -0.909 0.365015    \n#&gt; cylindernumbertwelve -1.101e+04  4.181e+03  -2.633 0.009386 ** \n#&gt; enginesize            1.257e+02  2.647e+01   4.749 4.89e-06 ***\n#&gt; fuelsystem2bbl        2.203e+02  8.779e+02   0.251 0.802254    \n#&gt; fuelsystemmfi        -3.116e+03  2.567e+03  -1.214 0.226855    \n#&gt; fuelsystemmpfi        4.302e+02  9.905e+02   0.434 0.664720    \n#&gt; fuelsystemspdi       -2.523e+03  1.360e+03  -1.856 0.065553 .  \n#&gt; fuelsystemspfi        5.316e+02  2.493e+03   0.213 0.831430    \n#&gt; boreratio            -1.219e+03  1.630e+03  -0.748 0.455731    \n#&gt; stroke               -4.583e+03  9.148e+02  -5.009 1.58e-06 ***\n#&gt; compressionratio     -7.785e+02  5.495e+02  -1.417 0.158716    \n#&gt; horsepower            1.139e+01  2.256e+01   0.505 0.614476    \n#&gt; peakrpm               2.522e+00  6.325e-01   3.988 0.000105 ***\n#&gt; highwaympg            7.601e+01  8.220e+01   0.925 0.356679    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2184 on 144 degrees of freedom\n#&gt; Multiple R-squared:  0.9414, Adjusted R-squared:  0.9267 \n#&gt; F-statistic: 64.24 on 36 and 144 DF,  p-value: &lt; 2.2e-16\n\n\nWe want to now consider seeing p-values for selecting significant regressors affecting price and we construct a barplot with alpha set to 0.05 and shown with red dotted line. All those variables with p-value greater than 0.05 we don’t consider them.\n\n#Extracting p-values of regressors from the summary of the regression model\np_values &lt;- summary(model)$coefficients[, \"Pr(&gt;|t|)\"]\np_values\n\n#&gt;          (Intercept)      aspirationturbo        doornumbertwo \n#&gt;         2.214873e-02         8.462557e-02         6.944711e-01 \n#&gt;       carbodyhardtop     carbodyhatchback         carbodysedan \n#&gt;         1.066928e-02         7.710289e-03         9.180131e-02 \n#&gt;         carbodywagon        drivewheelfwd        drivewheelrwd \n#&gt;         2.188745e-02         6.861656e-01         9.901899e-01 \n#&gt;   enginelocationrear            wheelbase            carlength \n#&gt;         7.658904e-03         6.622471e-01         6.359238e-01 \n#&gt;             carwidth            carheight           curbweight \n#&gt;         3.443999e-03         3.858277e-01         1.422933e-01 \n#&gt;      enginetypedohcv          enginetypel        enginetypeohc \n#&gt;         8.716763e-02         5.343980e-01         2.431410e-04 \n#&gt;       enginetypeohcf       enginetypeohcv   cylindernumberfive \n#&gt;         5.788177e-01         1.518253e-06         1.731842e-04 \n#&gt;   cylindernumberfour    cylindernumbersix  cylindernumberthree \n#&gt;         4.293199e-04         2.022597e-03         3.650153e-01 \n#&gt; cylindernumbertwelve           enginesize       fuelsystem2bbl \n#&gt;         9.386403e-03         4.887489e-06         8.022540e-01 \n#&gt;        fuelsystemmfi       fuelsystemmpfi       fuelsystemspdi \n#&gt;         2.268549e-01         6.647200e-01         6.555294e-02 \n#&gt;       fuelsystemspfi            boreratio               stroke \n#&gt;         8.314303e-01         4.557306e-01         1.576896e-06 \n#&gt;     compressionratio           horsepower              peakrpm \n#&gt;         1.587161e-01         6.144755e-01         1.054750e-04 \n#&gt;           highwaympg \n#&gt;         3.566786e-01\n\n# Extracting names of the regressors\nregressor_names &lt;- names(p_values)\n\n# Create a bar plot to visualize the significance of regressors\nbarplot(-log10(p_values), \n        names.arg = regressor_names,\n        main = \"Significance of Regressors on Car Prices\",\n        xlab = \"Regressors\",\n        ylab = \"-log10(p-value)\",\n        col = \"lightblue\",\n        ylim = c(0, max(-log10(p_values)) * 1.2),\n        las = 2, # Rotate x-axis labels vertically\n        cex.names = 0.7)\n\n# Add a horizontal dashed line at significance level (for example, p = 0.05)\nabline(h = -log10(0.05), lty = 2, col = \"red\")\n\n\n\n\n\n\n\n# Determine significance levels based on p-values\nsignificance_levels &lt;- ifelse(p_values &lt;= 0.001, \"***\",\n                              ifelse(p_values &lt;= 0.01, \"**\",\n                                     ifelse(p_values &lt;= 0.05, \"*\", \"\")))\n\n# Filter significant regressors whose p-values are less than 0.05\nsignificant_regressors &lt;- regressor_names[p_values &lt; 0.05]\n\n# Create a table of significant regressors along with their significance levels\n  significant_table &lt;- data.frame(\n  Regressors = significant_regressors,\n  P_Values = p_values[p_values &lt; 0.05],\n  Significance = significance_levels[p_values &lt; 0.05]\n)\nsignificant_table\n\n\n\n  \n\n\n# Include only significant regressors\nmodel_imp &lt;- lm(price ~ enginelocation + enginesize + stroke + carwidth + peakrpm, data = data)\nsummary(model_imp)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ enginelocation + enginesize + stroke + carwidth + \n#&gt;     peakrpm, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -9332  -1532   -235   1132  15349 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        -6.886e+04  1.055e+04  -6.529 6.91e-10 ***\n#&gt; enginelocationrear  1.100e+04  1.987e+03   5.536 1.11e-07 ***\n#&gt; enginesize          1.355e+02  8.905e+00  15.211  &lt; 2e-16 ***\n#&gt; stroke             -2.768e+03  7.194e+02  -3.847 0.000167 ***\n#&gt; carwidth            9.568e+02  1.671e+02   5.725 4.41e-08 ***\n#&gt; peakrpm             2.023e+00  5.634e-01   3.591 0.000427 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2948 on 175 degrees of freedom\n#&gt; Multiple R-squared:  0.8702, Adjusted R-squared:  0.8665 \n#&gt; F-statistic: 234.7 on 5 and 175 DF,  p-value: &lt; 2.2e-16\n\n# Extract the coefficients of significant regressors from the regression model summary\nsignificant_regressors &lt;- c(\"enginelocationrear\", \"enginesize\", \"stroke\", \"carwidth\", \"peakrpm\")\n\n# Extract the coefficients and variable names\ncoefficients &lt;- coef(model)[significant_regressors]\n\n# Create a bar plot to visualize the impact of significant regressors on price\nbarplot(coefficients, \n        main = \"Impact of Significant Regressors on Car Prices\",\n        xlab = \"Regressors\",\n        ylab = \"Coefficient Estimate\",\n        col = \"skyblue\",\n        ylim = c(min(coefficients) * 1.2, max(coefficients) * 1.2),\n        las = 2, # Rotate x-axis labels vertically\n        beside = TRUE)\n\n# Add labels for the variables\ntext(1:length(coefficients), coefficients, labels = round(coefficients, digits = 2), pos = 3, cex = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nNow for this part enginesize is selected based on its p-value (4.887489e-06) and significance and also a positive correletion based on the plot and positive estimate of 125.72 show that as enginesize increases, price also increases. The data type of enginesize is  double and it is statistically significant since its p-value is less than alpha= 0.05.\n\n# Plot relationship between enginesize and price\nggplot(data, aes(x = enginesize, y = price)) +\n  geom_point(alpha = 0.8)\n\n\n\n\n\n\n\n\n\n# Step 5: Add a new variable 'seat_heating' and run regression\ndata_with_seat_heating &lt;- data %&gt;% mutate(seat_heating = TRUE)\nmodel_with_seat_heating &lt;- lm(price ~ ., data = data_with_seat_heating)\nsummary(model_with_seat_heating) # Check regression summary for the coefficient of seat_heating\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = data_with_seat_heating)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; seat_heatingTRUE             NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nAdding a variable seat_heating to the data and assigning a value TRUE for all observations gave a NA for Estimate coefficient in the new regression model. This new variable has no affect on the price since it is same for all observations."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "#load libraries\nlibrary(ggVennDiagram)\n\nDefine being on time as event T, being not on time as !T, having a change in scope as S and having no change in scope as !S\nThen, compute the following probabilities and the sum of all four probabilities.\nP(T∩S): Probability of being on time and having a change in scope.\nP(T∩!S): Probability of being on time and having no change in scope.\nP(!T∩S): Probability of not being on time and having a change in scope.\nP(!T∩!S): Probability of not being on time and having no change in scope.\n\n\nP(S): Probability of having a change in scope.\nP(!S): Probability of having no change in scope.\nP(T∣S): Probability of being on time having a change in scope.\nP(T∣!S): Probability of being on time having no change in scope.\nP(!T∣S): Probability of not being on time having a change in scope.\nP(!T∣!S): Probability of not being on time having no change in scope.\n\n\n\nP(T∩S)=P(S)×P(T∣S)\nP(T∩!S)=P(!S)×P(T∣!S)\nP(!T∩S)=P(S)×P(!T∣S)\nP(!T∩!S)=P(S)×P(!T∣!S)\n\n\n\n\ncalculate_probabilities &lt;- function(P_S, P_not_S, P_T_given_S, P_not_T_given_S, P_not_T_and_S, P_not_T_and_not_S) {\n    # Calculate probabilities using the general equations\n    P_T_and_S &lt;- P_S * P_T_given_S\n    P_T_and_not_S &lt;- P_not_S * P_T_given_not_S\n    P_not_T_and_S &lt;- P_S * P_not_T_given_S\n    P_not_T_and_not_S &lt;- P_not_S * P_not_T_given_not_S\n\n    # Calculate the sum of all probabilities\n    sum_all_probabilities &lt;- P_T_and_S + P_T_and_not_S + P_not_T_and_S + P_not_T_and_not_S\n\n    # Return the calculated probabilities and the sum\n    return(list(\n        P_T_and_S = P_T_and_S,\n        P_T_and_not_S = P_T_and_not_S,\n        P_not_T_and_S = P_not_T_and_S,\n        P_not_T_and_not_S = P_not_T_and_not_S,\n        Sum_all_probabilities = sum_all_probabilities\n    ))\n}\n\n\n\n\n\n\n\nProbability Tree\n\n\n\nP_S &lt;- 0.3\nP_not_S &lt;- 0.7\nP_T_given_S &lt;- 0.2\nP_not_T_given_S &lt;- 1 - P_T_given_S\nP_T_given_not_S &lt;- 0.6\nP_not_T_given_not_S &lt;- 1 - P_T_given_not_S\n\n\n\n\n\nprobabilities &lt;- calculate_probabilities(P_S, P_not_S, P_T_given_S, P_not_T_given_S, P_T_given_not_S, P_not_T_given_not_S)\n\n\n\n\n\nprobabilities$P_T_and_S  #P(T∩S)\n\n#&gt; [1] 0.06\n\nprobabilities$P_T_and_not_S #P(T∩!S)\n\n#&gt; [1] 0.42\n\nprobabilities$P_not_T_and_S #P(!T∩S)\n\n#&gt; [1] 0.24\n\nprobabilities$P_not_T_and_not_S #P(!T∩!S)\n\n#&gt; [1] 0.28\n\nprobabilities$Sum_all_probabilities # P(T∩S) + P(T∩!S) + P(!T∩S) + P(!T∩!S)\n\n#&gt; [1] 1"
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "#load libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n# Load the data from random_vars.rds file\ndata &lt;- readRDS(\"data/random_vars.rds\")  # Replace 'path_to_your_folder' with the actual path\n\n# View the loaded data\nView(data)\n\n# Compute expected value, variance, and standard deviation for each variable\nsummary_stats &lt;- data.frame(\n  Variable = colnames(data),\n  Expected_Value = sapply(data, mean),\n  Variance = sapply(data, var),\n  Standard_Deviation = sapply(data, sd)\n)\n\n# View summary statistics\nsummary_stats\n\n\n\n  \n\n\n\n\n\n\nStandard deviation provides insights into the variability of individual variables, comparing relationships between ‘age’ and ‘income’ would benefit more from measures like covariance and correlation to understand how these variables are related or associated with each other but comparing covariance is not easy. Where as in correlation, we get values between -1 and 1 which provides a standardized measure to interpret the strength and direction of linear relationship between age and income.\n\n\n\n\n# Compute expected value, variance, and standard deviation for each variable\nsummary_stats &lt;- data.frame(\n  Variable = colnames(data),\n  Covariance = sapply(data, function(x) sapply(data, function(y) cov(x, y))),\n  Correlation = sapply(data, function(x) sapply(data, function(y) cor(x, y)))\n)\n\n# View summary statistics\nsummary_stats\n\n\n\n  \n\n\n\nIt is easier to interpret correlation as compared to covarience as discussed above.\n\n\n\n\n# Calculate conditional expected values for income based on age ranges\nconditional_expected_value &lt;- data %&gt;%\n  summarize(\n    E_income_age_18 = mean(filter(., age &lt;= 18)$income),\n    E_income_age_18_65 = mean(filter(., age &gt;= 18 & age &lt; 65)$income),\n    E_income_age_65 = mean(filter(., age &gt;= 65)$income)\n  )\n\nconditional_expected_value"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "1 Assignment 4\n\n#load libraries\nlibrary(ggplot2)\n\nIt is common for unrelated variables to appear to be correlated by coincidence. This is known as spurious correlation. The “Spending on science, space, and technology” in the United States is one example of this, as it shows a surprisingly strong correlation with the “Suicides by hanging, strangulation and suffocation” over time. While there is a significant positive correlation between these two variables, there is no causal relationship.\nExample taken from (https://tylervigen.com/spurious-correlations)\n Generating random data to show spurious correlation\n\n# Generating random data\nset.seed(123)\nn &lt;- 100\nx &lt;- rnorm(n)  # Generating random data for variable X\ny &lt;- x + rnorm(n, mean = 0, sd = 0.2)  # Generating Y correlated with X\n\n# Create a dataframe\ndata &lt;- data.frame(X = x, Y = y)\n\n# Plotting the spurious correlation using ggplot2\nggplot(data, aes(x = X, y = Y)) +\n  geom_point() +\n  labs(title = \"Spurious Correlation\",\n       x = \"Variable X\",\n       y = \"Variable Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n“X” and “Y” are two random variables in this example. Random noise is added to ‘X’ to generate ‘Y’, resulting in a correlation between the two. ‘X’ and ‘Y’, however, have no meaningful relationship, despite their visual correlation in the plot. This illustrates how unrelated variables can still produce spurious correlations."
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\n\n\n\n\n# Loading dataset into 'data'\ndata &lt;- readRDS(\"data/abtest_online.rds\")\nstr(data)\n\n#&gt; tibble [1,000 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ ip             : chr [1:1000] \"161.88.211.70\" \"239.86.201.0\" \"35.90.22.130\" \"219.176.193.3\" ...\n#&gt;  $ chatbot        : logi [1:1000] TRUE TRUE TRUE FALSE TRUE TRUE ...\n#&gt;  $ previous_visit : num [1:1000] 0 1 1 4 1 2 1 1 1 0 ...\n#&gt;  $ mobile_device  : logi [1:1000] FALSE FALSE FALSE TRUE FALSE FALSE ...\n#&gt;  $ purchase       : num [1:1000] 0 1 0 0 0 0 1 1 0 0 ...\n#&gt;  $ purchase_amount: num [1:1000] 0 39.5 0 0 0 ...\n\n# Convert non-numeric variables to numeric\ndata$mobile_device &lt;- as.numeric(data$mobile_device)\ndata$chatbot &lt;- as.numeric(data$chatbot)\n\n# Check covariate balance across groups using a plot \ncovariates &lt;- c(\"mobile_device\", \"previous_visit\", \"purchase\", \"purchase_amount\") # Variables of interest\ngroup_var &lt;- \"chatbot\"\n\n# Check covariate balance visually using plots\nplot_balances &lt;- lapply(covariates, function(var) {\n  ggplot(data, aes_string(x = group_var, y = var, color = as.factor(group_var))) +\n    stat_summary(\n      geom = \"errorbar\", \n      width = 0.5,\n      fun.data = \"mean_se\", \n      fun.args = list(mult = 1.96),\n      show.legend = FALSE\n    ) +\n    labs(title = paste(\"Covariate Balance for\", var))\n})\n\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\nplot_balances\n\n#&gt; [[1]]\n\n\n\n\n\n\n\n\n\n#&gt; \n#&gt; [[2]]\n\n\n\n\n\n\n\n\n\n#&gt; \n#&gt; [[3]]\n\n\n\n\n\n\n\n\n\n#&gt; \n#&gt; [[4]]\n\n\n\n\n\n\n\n\n\nPlots shows in general, covariates are overall balanced when chatbot is True and False. However, when comparing previous_visits plots it can be seen that the CI(Confidence Intervals) overlap with each other.\n\n\n\n\nsales_model &lt;- lm(purchase_amount ~ chatbot, data = data)\nsummary(sales_model) # regression summary\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbot      -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\nThe summary shows that the sales dropped for customers that used the chatbot with the chatbot coefficient value = -7.0756\n\n\n\n\ninteraction_model &lt;- lm(purchase_amount ~ chatbot * mobile_device, data = data)\nsummary(interaction_model) # regression summary (-0.1526 for chatbot:mobile_device)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -16.98 -14.54  -9.95  14.13  65.24 \n#&gt; \n#&gt; Coefficients:\n#&gt;                       Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            16.9797     1.0152  16.725   &lt;2e-16 ***\n#&gt; chatbot                -7.0301     1.4284  -4.922    1e-06 ***\n#&gt; mobile_device          -0.8727     1.7987  -0.485    0.628    \n#&gt; chatbot:mobile_device  -0.1526     2.5369  -0.060    0.952    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.66 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.03534,    Adjusted R-squared:  0.03244 \n#&gt; F-statistic: 12.16 on 3 and 996 DF,  p-value: 8.034e-08\n\n# Computing CATE for mobile users (subgroup)\nmobile_users &lt;- data[data$mobile_device == 1, ]\n\n# Fit a model for mobile users\nmobile_users_model &lt;- lm(purchase_amount ~ chatbot, data = mobile_users)\nsummary(mobile_users_model) # View regression summary\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = mobile_users)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.107 -16.107  -8.924  10.594  65.243 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   16.107      1.498  10.751  &lt; 2e-16 ***\n#&gt; chatbot       -7.183      2.115  -3.395 0.000773 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.83 on 315 degrees of freedom\n#&gt; Multiple R-squared:  0.03531,    Adjusted R-squared:  0.03225 \n#&gt; F-statistic: 11.53 on 1 and 315 DF,  p-value: 0.0007729\n\n\n\n\n\n\nlogistic_model &lt;- glm(purchase ~ chatbot, \n                      family = binomial(link = 'logit'), data = data)\nsummary(logistic_model) # View regression summary\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot, family = binomial(link = \"logit\"), \n#&gt;     data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.01613    0.08981  -0.180    0.857    \n#&gt; chatbot     -0.98939    0.13484  -7.337 2.18e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1273.3  on 998  degrees of freedom\n#&gt; AIC: 1277.3\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Now Interpreting the coefficient for 'chatbot'\n# Coefficients here represent log odds ratio. In order to get the odds ratio, coefficient for chatbot is exponentiated.\nexp(coef(logistic_model)[\"chatbot\"]) # Exponentiated coefficient for 'chatbot'\n\n#&gt;   chatbot \n#&gt; 0.3718025\n\n\nThe odds ratio amount to 0.371 which suggests that the customers are less likely to purchase something with the chatbot\n```"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "library(Matching)\nlibrary(ggdag)\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(MatchIt)\n\n# loading data into data\ndata &lt;- readRDS(\"data/hospdd.rds\")\ndata\n\n\n\n  \n\n\nnames(data)\n\n#&gt; [1] \"hospital\"  \"frequency\" \"month\"     \"procedure\" \"satis\"\n\nmean_before_treatment &lt;- data %&gt;%\n  filter(procedure == 0, hospital&gt;= 1 & hospital&lt;=18) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_after_treatment &lt;- data %&gt;%\n  filter(procedure == 1, hospital&gt;= 1 & hospital&lt;=18) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_before_control &lt;- data %&gt;%\n  filter(procedure == 0, hospital&gt;18 & hospital&lt;= 46) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_after_control &lt;- data %&gt;%\n  filter(procedure == 0, hospital&gt;18 & hospital&lt;= 46) %&gt;%\n  pull(satis) %&gt;%\n  mean()\ndiff_before &lt;- mean_before_treatment - mean_before_control\ndiff_after &lt;- mean_after_treatment - mean_after_control\n\ndiff_in_diff &lt;- diff_after - diff_before\n\nsprintf(\"Mean satisfaction for treated hospitals before treatment: %f\" ,mean_before_treatment)\n\n#&gt; [1] \"Mean satisfaction for treated hospitals before treatment: 3.525383\"\n\nsprintf(\"Mean satisfaction for treated hospitals after treatment: %f\", mean_after_treatment)\n\n#&gt; [1] \"Mean satisfaction for treated hospitals after treatment: 4.363351\"\n\nsprintf(\"Mean satisfaction for control hospitals before treatment: %f\" , mean_before_control)\n\n#&gt; [1] \"Mean satisfaction for control hospitals before treatment: 3.387499\"\n\nsprintf(\"Mean satisfaction for control hospitals after treatment: %f\", mean_after_control)\n\n#&gt; [1] \"Mean satisfaction for control hospitals after treatment: 3.387499\"\n\nsprintf(\"Difference in difference: %f\", diff_in_diff)\n\n#&gt; [1] \"Difference in difference: 0.837968\"\n\n\n\n\n\n\n# Linear regression with month and hospital as continuous variables\nmodel_continuous &lt;- lm(satis ~ month + hospital + procedure, data = data)\nsummary(model_continuous)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ month + hospital + procedure, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.3126 -0.6548 -0.0933  0.5555  5.3347 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  3.538024   0.031365 112.801  &lt; 2e-16 ***\n#&gt; month       -0.004965   0.006392  -0.777 0.437378    \n#&gt; hospital    -0.003731   0.001043  -3.576 0.000351 ***\n#&gt; procedure    0.886120   0.039143  22.638  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9836 on 7364 degrees of freedom\n#&gt; Multiple R-squared:  0.1325, Adjusted R-squared:  0.1321 \n#&gt; F-statistic: 374.8 on 3 and 7364 DF,  p-value: &lt; 2.2e-16\n\n# Linear regression with factors for month and hospital\nmodel_factors &lt;- lm(satis ~ as.factor(month) + as.factor(hospital) + procedure, data = data)\nsummary(model_factors)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(month) + as.factor(hospital) + \n#&gt;     procedure, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.1880 -0.4644  0.0067  0.4539  4.2921 \n#&gt; \n#&gt; Coefficients:\n#&gt;                         Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            3.1716566  0.0562207  56.414  &lt; 2e-16 ***\n#&gt; as.factor(month)2     -0.0096077  0.0292119  -0.329 0.742244    \n#&gt; as.factor(month)3      0.0219686  0.0292119   0.752 0.452050    \n#&gt; as.factor(month)4     -0.0032839  0.0324936  -0.101 0.919504    \n#&gt; as.factor(month)5     -0.0094027  0.0324936  -0.289 0.772305    \n#&gt; as.factor(month)6     -0.0038375  0.0324936  -0.118 0.905990    \n#&gt; as.factor(month)7     -0.0111941  0.0324936  -0.345 0.730478    \n#&gt; as.factor(hospital)2   0.4085664  0.0772418   5.289 1.26e-07 ***\n#&gt; as.factor(hospital)3   0.5336248  0.0793384   6.726 1.88e-11 ***\n#&gt; as.factor(hospital)4   0.2275102  0.0739411   3.077 0.002099 ** \n#&gt; as.factor(hospital)5  -0.1453529  0.0739411  -1.966 0.049360 *  \n#&gt; as.factor(hospital)6   0.4478634  0.0739411   6.057 1.46e-09 ***\n#&gt; as.factor(hospital)7   1.4044164  0.0714559  19.654  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8   0.0718758  0.0763186   0.942 0.346333    \n#&gt; as.factor(hospital)9  -1.5185150  0.0782447 -19.407  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10  1.6828446  0.0772418  21.787  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11  0.2209653  0.0763186   2.895 0.003799 ** \n#&gt; as.factor(hospital)12 -0.0953034  0.0782447  -1.218 0.223256    \n#&gt; as.factor(hospital)13  0.4955931  0.0754658   6.567 5.48e-11 ***\n#&gt; as.factor(hospital)14  0.2330426  0.0793384   2.937 0.003321 ** \n#&gt; as.factor(hospital)15 -0.1444935  0.0793384  -1.821 0.068613 .  \n#&gt; as.factor(hospital)16  1.4142680  0.0772418  18.310  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17  0.4235429  0.0805362   5.259 1.49e-07 ***\n#&gt; as.factor(hospital)18  0.1532761  0.0938164   1.634 0.102346    \n#&gt; as.factor(hospital)19 -0.7453017  0.0811623  -9.183  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20  0.0473874  0.0791140   0.599 0.549207    \n#&gt; as.factor(hospital)21  1.1943370  0.0836232  14.282  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22  0.7993153  0.0823336   9.708  &lt; 2e-16 ***\n#&gt; as.factor(hospital)23  0.7017202  0.0811623   8.646  &lt; 2e-16 ***\n#&gt; as.factor(hospital)24 -0.3081260  0.0866402  -3.556 0.000378 ***\n#&gt; as.factor(hospital)25  0.6464736  0.0927258   6.972 3.40e-12 ***\n#&gt; as.factor(hospital)26  0.2142471  0.0791140   2.708 0.006783 ** \n#&gt; as.factor(hospital)27 -0.3986544  0.0766106  -5.204 2.01e-07 ***\n#&gt; as.factor(hospital)28  0.7119953  0.0836232   8.514  &lt; 2e-16 ***\n#&gt; as.factor(hospital)29  0.2485512  0.0800935   3.103 0.001921 ** \n#&gt; as.factor(hospital)30 -0.1679220  0.0953638  -1.761 0.078304 .  \n#&gt; as.factor(hospital)31  0.5120848  0.0791140   6.473 1.02e-10 ***\n#&gt; as.factor(hospital)32 -0.3233456  0.0800935  -4.037 5.47e-05 ***\n#&gt; as.factor(hospital)33 -0.4539752  0.0791140  -5.738 9.95e-09 ***\n#&gt; as.factor(hospital)34 -0.0004123  0.0746054  -0.006 0.995590    \n#&gt; as.factor(hospital)35  0.3541110  0.0766106   4.622 3.86e-06 ***\n#&gt; as.factor(hospital)36  2.1381425  0.0773811  27.631  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37  0.1404036  0.0927258   1.514 0.130023    \n#&gt; as.factor(hospital)38 -0.0868060  0.0782129  -1.110 0.267093    \n#&gt; as.factor(hospital)39 -0.0234969  0.0823336  -0.285 0.775356    \n#&gt; as.factor(hospital)40  1.1215331  0.0782129  14.339  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41 -0.1497346  0.0766106  -1.954 0.050681 .  \n#&gt; as.factor(hospital)42  0.8811369  0.0850508  10.360  &lt; 2e-16 ***\n#&gt; as.factor(hospital)43 -0.7724325  0.0811623  -9.517  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44  0.0344120  0.0904337   0.381 0.703569    \n#&gt; as.factor(hospital)45 -0.2137495  0.0766106  -2.790 0.005283 ** \n#&gt; as.factor(hospital)46  0.0784915  0.0823336   0.953 0.340452    \n#&gt; procedure              0.8479879  0.0342191  24.781  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7238 on 7315 degrees of freedom\n#&gt; Multiple R-squared:  0.5333, Adjusted R-squared:  0.5299 \n#&gt; F-statistic: 160.7 on 52 and 7315 DF,  p-value: &lt; 2.2e-16\n\n\nThe model using categorical variables for ‘month’ and ‘hospital’ demonstrates a better fit explaining 52.99% of the variance in patient satisfaction and improved explanatory power compared to the continuous variable model. Individual hospitals and certain months have significant impacts on patient satisfaction besides the effect of the procedure."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "#load libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rddensity)\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ lubridate 1.9.3     ✔ tibble    3.2.1\n#&gt; ✔ purrr     1.0.2     ✔ tidyr     1.3.0\n#&gt; ✔ readr     2.1.4     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read data for coupon.rds\ndf &lt;- readRDS(\"data/coupon.rds\")\n\n# Define cut-off\nc0 &lt;- 60\nbw&lt;- c0+c(-2.5,2.5)\ndf\n\n\n\n  \n\n\n# [2.2] Random assignment test for half bandwidth ----\nggplot(df, aes(x = days_since_last, fill = coupon)) +\n  geom_histogram(binwidth = 4, color = \"white\", alpha = 0.6) +\n  geom_vline(xintercept = c0 - 30, color = \"red\", linetype = \"dashed\") + # Half bandwidth\n  scale_fill_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Number of customers\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n# [3.2] LATE for half bandwidth ----\ndf_below_half &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_above_half &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\nmodel_below_half &lt;- lm(purchase_after ~ days_since_last, data = df_below_half)\nmodel_above_half &lt;- lm(purchase_after ~ days_since_last, data = df_above_half)\n\npred_below_half &lt;- predict(model_below_half, tibble(days_since_last = c0))\npred_above_half &lt;- predict(model_above_half, tibble(days_since_last = c0))\n\nlate_half &lt;- pred_above_half - pred_below_half\nsprintf(\"LATE with half bandwidth: %.2f\", late_half)\n\n#&gt; [1] \"LATE with half bandwidth: 7.36\"\n\n# [4.1] Estimation with half bandwidth ----\nlm_bw_half &lt;- lm(purchase_after ~ days_since_last, data = df)\nsummary(lm_bw_half)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -14.244  -3.620  -0.558   2.868  34.353 \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     12.353269   0.117865  104.81   &lt;2e-16 ***\n#&gt; days_since_last  0.053425   0.002281   23.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.331 on 4998 degrees of freedom\n#&gt; Multiple R-squared:  0.09889,    Adjusted R-squared:  0.09871 \n#&gt; F-statistic: 548.5 on 1 and 4998 DF,  p-value: &lt; 2.2e-16\n\nggplot(df, aes(x = days_since_last, y = purchase_after)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_vline(xintercept = c0 - 30, linetype = \"dashed\", color = \"red\") + # Half bandwidth\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after campaign\") +\n  ggtitle(\"Regression lines for half bandwidth\")\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# [2.2] Random assignment test for double bandwidth ----\nggplot(df, aes(x = days_since_last, fill = coupon)) +\n  geom_histogram(binwidth = 4, color = \"white\", alpha = 0.6) +\n  geom_vline(xintercept = c0 + 30, color = \"red\", linetype = \"dashed\") + # Double bandwidth\n  scale_fill_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Number of customers\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n# [3.2] LATE for double bandwidth ----\nbw &lt;- c0+c(-10,10)\ndf_below_double &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_above_double &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\nmodel_below_double &lt;- lm(purchase_after ~ days_since_last, data = df_below_double)\nmodel_above_double &lt;- lm(purchase_after ~ days_since_last, data = df_above_double)\n\npred_below_double &lt;- predict(model_below_double, tibble(days_since_last = c0))\npred_above_double &lt;- predict(model_above_double, tibble(days_since_last = c0))\n\nlate_double &lt;- mean(pred_above_double) - mean(pred_below_double)\nsprintf(\"LATE with double bandwidth: %.2f\", late_double)\n\n#&gt; [1] \"LATE with double bandwidth: 9.51\"\n\n# [4.1] Estimation with double bandwidth ----\nlm_bw_double &lt;- lm(purchase_after ~ days_since_last, data = df)\nsummary(lm_bw_double)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -14.244  -3.620  -0.558   2.868  34.353 \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     12.353269   0.117865  104.81   &lt;2e-16 ***\n#&gt; days_since_last  0.053425   0.002281   23.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.331 on 4998 degrees of freedom\n#&gt; Multiple R-squared:  0.09889,    Adjusted R-squared:  0.09871 \n#&gt; F-statistic: 548.5 on 1 and 4998 DF,  p-value: &lt; 2.2e-16\n\nggplot(df, aes(x = days_since_last, y = purchase_after)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_vline(xintercept = c0 + 30, linetype = \"dashed\", color = \"red\") + # Double bandwidth\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after campaign\") +\n  ggtitle(\"Regression lines for double bandwidth\")\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAfter running the analysis it was noticed that making the bandwidth half resulted in LATE value of 7.53. Where as, doubling the bandwidth resulted in a higher LATE value of 9.51.\n\n\n\n\n# Read data for shipping.rds\nshipping_df &lt;- readRDS(\"data/shipping.rds\")\nshipping_df\n\n\n\n  \n\n\n# Plot to evaluate 'purchase_amount' as a running variable with a cutoff at 30€\nggplot(shipping_df, aes(x = purchase_amount, fill = purchase_amount &gt; 30)) +\n  geom_histogram(binwidth = 5, color = \"white\", alpha = 0.6) +\n  scale_fill_manual(values = c(\"blue\", \"red\"), labels = c(\"&lt;= 30€\", \"&gt; 30€\")) +\n  geom_vline(xintercept = 30, linetype = \"dashed\", color = \"black\") +\n  xlab(\"Purchase Amount (€)\") +\n  ylab(\"Number of Purchases\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nFor RDD to deliver valid results we have to make sure there is no non-random heaping at the cut-off, i.e. no manipulation because for example the effect is known and units attempt to move to one side of the cut-off. We can plot the distribution around the cut-off to check for violations of the continuity assumption.\nWe can see that there is indeed decline or incline at the cut-off and therefore we cannot assume that the continuity assumption holds.\nTo check the continuity assumption more thoroughly, we can also use functions of the rddensity package\n\n# Density test\n# Check for continuous density along running variable. Manipulations could \n# lead to running variable being \"crowded\" right after cutoff.\nrddd &lt;- rddensity(shipping_df$purchase_amount, c = 30)\nsummary(rddd)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0\n\n\n#&gt; Warning in summary.CJMrddensity(rddd): There are repeated observations. Point\n#&gt; estimates and standard errors have been adjusted. Use option massPoints=FALSE\n#&gt; to suppress this feature.\n\n\n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\n# Visually check continuity at running variable\nrdd_plot &lt;- rdplotdensity(rddd, shipping_df$purchase_amount, plotN = 100)\n\n\n\n\n\n\n\n\nThe plot confirms our assumption. we can see that the confidence intervals do not overlap. Since they do not overlap, we would have to suspect some kind of manipulation around the cut-off and could not use RDD to obtain valid results."
  },
  {
    "objectID": "content/01_journal/01_probability.html#function-to-calculate-probabilities-based-on-provided-inputs",
    "href": "content/01_journal/01_probability.html#function-to-calculate-probabilities-based-on-provided-inputs",
    "title": "Probability Theory",
    "section": "",
    "text": "calculate_probabilities &lt;- function(P_S, P_not_S, P_T_given_S, P_not_T_given_S, P_not_T_and_S, P_not_T_and_not_S) {\n    # Calculate probabilities using the general equations\n    P_T_and_S &lt;- P_S * P_T_given_S\n    P_T_and_not_S &lt;- P_not_S * P_T_given_not_S\n    P_not_T_and_S &lt;- P_S * P_not_T_given_S\n    P_not_T_and_not_S &lt;- P_not_S * P_not_T_given_not_S\n\n    # Calculate the sum of all probabilities\n    sum_all_probabilities &lt;- P_T_and_S + P_T_and_not_S + P_not_T_and_S + P_not_T_and_not_S\n\n    # Return the calculated probabilities and the sum\n    return(list(\n        P_T_and_S = P_T_and_S,\n        P_T_and_not_S = P_T_and_not_S,\n        P_not_T_and_S = P_not_T_and_S,\n        P_not_T_and_not_S = P_not_T_and_not_S,\n        Sum_all_probabilities = sum_all_probabilities\n    ))\n}"
  },
  {
    "objectID": "content/01_journal/01_probability.html#now-to-calculate-the-probabilities-example-values-can-be-taken-from-probability-tree-to-compute-the-probabilities",
    "href": "content/01_journal/01_probability.html#now-to-calculate-the-probabilities-example-values-can-be-taken-from-probability-tree-to-compute-the-probabilities",
    "title": "Probability Theory",
    "section": "",
    "text": "P_S &lt;- 0.3\nP_not_S &lt;- 0.7\nP_T_given_S &lt;- 0.2\nP_not_T_given_S &lt;- 1 - P_T_given_S\nP_T_given_not_S &lt;- 0.6\nP_not_T_given_not_S &lt;- 1 - P_T_given_not_S"
  },
  {
    "objectID": "content/01_journal/01_probability.html#calculate-probabilities-using-the-function",
    "href": "content/01_journal/01_probability.html#calculate-probabilities-using-the-function",
    "title": "Probability Theory",
    "section": "",
    "text": "probabilities &lt;- calculate_probabilities(P_S, P_not_S, P_T_given_S, P_not_T_given_S, P_T_given_not_S, P_not_T_given_not_S)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#print-the-calculated-probabilities",
    "href": "content/01_journal/01_probability.html#print-the-calculated-probabilities",
    "title": "Probability Theory",
    "section": "",
    "text": "print(probabilities$P_T_and_S)\n\n#&gt; [1] 0.28\n\nprint(probabilities$P_T_and_not_S)\n\n#&gt; [1] 0.42\n\nprint(probabilities$P_not_T_and_S)\n\n#&gt; [1] 0.09\n\nprint(probabilities$P_not_T_and_not_S)\n\n#&gt; [1] 0.21\n\nprint(probabilities$Sum_all_probabilities)\n\n#&gt; [1] 1"
  },
  {
    "objectID": "content/01_journal/01_probability.html#calculated-probabilities",
    "href": "content/01_journal/01_probability.html#calculated-probabilities",
    "title": "Probability Theory",
    "section": "",
    "text": "probabilities$P_T_and_S\n\n#&gt; [1] 0.06\n\nprobabilities$P_T_and_not_S\n\n#&gt; [1] 0.42\n\nprobabilities$P_not_T_and_S\n\n#&gt; [1] 0.24\n\nprobabilities$P_not_T_and_not_S\n\n#&gt; [1] 0.28\n\nprobabilities$Sum_all_probabilities\n\n#&gt; [1] 1"
  },
  {
    "objectID": "content/01_journal/01_probability.html#given-the-following-definitions",
    "href": "content/01_journal/01_probability.html#given-the-following-definitions",
    "title": "Probability Theory",
    "section": "",
    "text": "P(S): Probability of having a change in scope.\nP(!S): Probability of having no change in scope.\nP(T∣S): Probability of being on time having a change in scope.\nP(T∣!S): Probability of being on time having no change in scope.\nP(!T∣S): Probability of not being on time having a change in scope.\nP(!T∣!S): Probability of not being on time having no change in scope."
  },
  {
    "objectID": "content/01_journal/01_probability.html#the-general-equations-for-the-joint-probabilities-are",
    "href": "content/01_journal/01_probability.html#the-general-equations-for-the-joint-probabilities-are",
    "title": "Probability Theory",
    "section": "",
    "text": "P(T∩S)=P(S)×P(T∣S)\nP(T∩!S)=P(!S)×P(T∣!S)\nP(!T∩S)=P(S)×P(!T∣S)\nP(!T∩!S)=P(S)×P(!T∣!S)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#calculated-joint-probabilities-and-sum-of-all-probabilities",
    "href": "content/01_journal/01_probability.html#calculated-joint-probabilities-and-sum-of-all-probabilities",
    "title": "Probability Theory",
    "section": "",
    "text": "probabilities$P_T_and_S\n\n#&gt; [1] 0.06\n\nprobabilities$P_T_and_not_S\n\n#&gt; [1] 0.42\n\nprobabilities$P_not_T_and_S\n\n#&gt; [1] 0.24\n\nprobabilities$P_not_T_and_not_S\n\n#&gt; [1] 0.28\n\nprobabilities$Sum_all_probabilities\n\n#&gt; [1] 1"
  },
  {
    "objectID": "content/01_journal/01_probability.html#now-to-calculate-the-probabilities-example-values-are-taken-from-probability-tree-to-compute-the-probabilities",
    "href": "content/01_journal/01_probability.html#now-to-calculate-the-probabilities-example-values-are-taken-from-probability-tree-to-compute-the-probabilities",
    "title": "Probability Theory",
    "section": "",
    "text": "Probability Tree\n\n\n\nP_S &lt;- 0.3\nP_not_S &lt;- 0.7\nP_T_given_S &lt;- 0.2\nP_not_T_given_S &lt;- 1 - P_T_given_S\nP_T_given_not_S &lt;- 0.6\nP_not_T_given_not_S &lt;- 1 - P_T_given_not_S"
  },
  {
    "objectID": "content/01_journal/01_probability.html#calculating-probabilities-using-the-function",
    "href": "content/01_journal/01_probability.html#calculating-probabilities-using-the-function",
    "title": "Probability Theory",
    "section": "",
    "text": "probabilities &lt;- calculate_probabilities(P_S, P_not_S, P_T_given_S, P_not_T_given_S, P_T_given_not_S, P_not_T_given_not_S)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#calculating-joint-probabilities-and-sum-of-all-probabilities",
    "href": "content/01_journal/01_probability.html#calculating-joint-probabilities-and-sum-of-all-probabilities",
    "title": "Probability Theory",
    "section": "",
    "text": "probabilities$P_T_and_S  #P(T∩S)\n\n#&gt; [1] 0.06\n\nprobabilities$P_T_and_not_S #P(T∩!S)\n\n#&gt; [1] 0.42\n\nprobabilities$P_not_T_and_S #P(!T∩S)\n\n#&gt; [1] 0.24\n\nprobabilities$P_not_T_and_not_S #P(!T∩!S)\n\n#&gt; [1] 0.28\n\nprobabilities$Sum_all_probabilities # P(T∩S) + P(T∩!S) + P(!T∩S) + P(!T∩!S)\n\n#&gt; [1] 1"
  },
  {
    "objectID": "content/01_journal/01_probability.html#venn-diagram-code",
    "href": "content/01_journal/01_probability.html#venn-diagram-code",
    "title": "Probability Theory",
    "section": "2.1 Venn Diagram Code",
    "text": "2.1 Venn Diagram Code\n\n# Number of obervations\nn &lt;- 1000\n\n# Create tibble\napp_usage &lt;- tibble(\n  # Create user_id in increasing order\n  user_id = 1:n,\n  # Randomly sample if smartphone was used\n  smartphone = rbinom(n, 1, 0.4),\n  # Sample if tablet was used. More likely if smartphone was not used.\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  # Sample if computer was used. More likely if tablet was not used.\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n# If no device has value of 1, we set smartphone to 1\napp_usage &lt;- app_usage %&gt;%\n  rowwise() %&gt;% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\nhead(app_usage, 10)\n\n\n\n  \n\n\n# Show column sums\ncolSums(app_usage)\n\n#&gt;    user_id smartphone     tablet   computer \n#&gt;     500500        610        367        242\n\n# Equivalent commands to select specific columns\n#colSums(app_usage[, 2:4])\n#colSums(app_usage[, c(\"smartphone\", \"tablet\", \"computer\")])\napp_usage %&gt;% select(smartphone, tablet, computer) %&gt;% colSums()\n\n#&gt; smartphone     tablet   computer \n#&gt;        610        367        242\n\n# Set of phone, tablet and computer users\nset_phon &lt;- which(app_usage$smartphone == 1)\nset_tabl &lt;- which(app_usage$tablet == 1)\nset_comp &lt;- which(app_usage$computer == 1)\n\n# List of all sets\nsets_all &lt;- list(set_phon, set_tabl, set_comp)\n\n# Load additional package for plotting Venn diagrams\n\n# Plot Venn diagram\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customizing appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\n# List of all sets\nsets_all &lt;- list(set_phon, set_tabl, set_comp)\n# Percentage of customers using all three devices\ncustomers_using_all_three &lt;- Reduce(intersect, sets_all)\npercentage_using_all_three &lt;- length(customers_using_all_three) / n * 100\n\n\n\n# Customers using at least two devices\ncustomers_using_at_least_two &lt;- (length(Reduce(intersect, sets_all[c(1, 3)])) +\n                                 length(Reduce(intersect, sets_all[c(1, 2)])) +\n                                 length(Reduce(intersect, sets_all[c(2, 3)])) -\n                                 2 * length(customers_using_all_three))\n\npercentage_using_at_least_two &lt;- customers_using_at_least_two / n * 100\n\n# Customers using only one device\ncustomers_using_only_one &lt;- (length(set_phon) + length(set_tabl) + length(set_comp) -\n                              2 * length(Reduce(intersect, sets_all[c(1, 3)])) -\n                              2 * length(Reduce(intersect, sets_all[c(1, 2)])) -\n                              2 * length(Reduce(intersect, sets_all[c(2, 3)])) +\n                              3 * length(customers_using_all_three))\n\npercentage_using_only_one &lt;- customers_using_only_one / n * 100"
  },
  {
    "objectID": "content/01_journal/01_probability.html#answers-generated-from-the-above-venn-diagram",
    "href": "content/01_journal/01_probability.html#answers-generated-from-the-above-venn-diagram",
    "title": "Probability Theory",
    "section": "2.2 Answers generated from the above Venn diagram",
    "text": "2.2 Answers generated from the above Venn diagram\n\n# Percentage of customers using all three devices (A∩B∩C)/n\npercentage_using_all_three\n\n#&gt; [1] 0.6\n\n# Percentage of customers using at least two devices ((A∩B)+(A∩C)+(B∩C)-2*(A∩B∩C))/n\npercentage_using_at_least_two\n\n#&gt; [1] 21.3\n\n# Percentage of customers using only one device (|A|+|B|+|C|-2*(A∩B)-2*(A∩C)-2*(B∩C)+3*(A∩B∩C))/n\npercentage_using_only_one\n\n#&gt; [1] 78.7\n\n\n\n\n\nCourse Website Venn Diagram"
  },
  {
    "objectID": "content/01_journal/01_probability.html#answers-generated-from-the-course-websites-venn-diagram",
    "href": "content/01_journal/01_probability.html#answers-generated-from-the-course-websites-venn-diagram",
    "title": "Probability Theory",
    "section": "2.3 Answers generated from the Course Website’s Venn diagram",
    "text": "2.3 Answers generated from the Course Website’s Venn diagram\nPercentage of customers using all three devices = 0.5\nPercentage of customers using at least two devices = 19.9\nPercentage of customers using only one device = 80.1"
  },
  {
    "objectID": "content/01_journal/05_dag.html#assignment-5.1",
    "href": "content/01_journal/05_dag.html#assignment-5.1",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "# Define the DAG for the parking spot example\nparking_dag &lt;- dagify(\n  sales ~ parking_spots,\n  sales ~ location,\n  parking_spots ~ location,\n  coords = list(x = c(sales = 1, parking_spots = 3, location = 2),\n                y = c(sales = 0, parking_spots = 0, location = 1))\n)\n\n\n# Plot the DAG with default ggplot2 theme\nggdag(parking_dag, text = FALSE) +\n  geom_dag_point() +\n  geom_dag_text(color = \"red\") +\n  geom_dag_edges(edge_color = \"black\")"
  },
  {
    "objectID": "content/01_journal/05_dag.html#assignment-5.2.1",
    "href": "content/01_journal/05_dag.html#assignment-5.2.1",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "# Load the data file customer_sat.rds\ndata &lt;- readRDS(\"data/customer_sat.rds\")\n\n# 1. Regress satisfaction on follow_ups\nmodel_1 &lt;- lm(satisfaction ~ follow_ups, data = data)\n\nsummary(model_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427"
  },
  {
    "objectID": "content/01_journal/05_dag.html#assignment-5.2.2",
    "href": "content/01_journal/05_dag.html#assignment-5.2.2",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "# 2. Regress satisfaction on follow_ups and account for subscription\nmodel_2 &lt;- lm(satisfaction ~ follow_ups + subscription, data = data)\n\nsummary(model_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08"
  },
  {
    "objectID": "content/01_journal/05_dag.html#assignment-5.3",
    "href": "content/01_journal/05_dag.html#assignment-5.3",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "According to the first regression, there is a negative relationship between follow_ups and satisfaction, indicating that as the number of follow-up calls increases, so does satisfaction.\nThe second regression, which takes into account both follow_ups and subscription levels, reveals that, while follow_ups continue to have an impact on satisfaction, subscription levels (Premium and Premium+) also have a significant impact. Furthermore, the subscription level coefficients imply that higher subscription tiers (Premium and Premium+) have a positive impact on satisfaction when compared to some assumed base level (possibly Starter, which is not explicitly included as a variable)."
  },
  {
    "objectID": "content/01_journal/05_dag.html#assignment-5.4",
    "href": "content/01_journal/05_dag.html#assignment-5.4",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "# Create a DAG for the relationship between follow-up calls, satisfaction, and subscription\nsaas_dag &lt;- dagify(\n  satisfaction ~ follow_ups,\n  satisfaction ~ subscription,\n  follow_ups ~ subscription,\n  coords = list(x = c(satisfaction = 1, follow_ups = 3, subscription = 2),\n                y = c(satisfaction = 0, follow_ups = 0, subscription = 1))\n)\n\n# Plot the DAG \nggdag(saas_dag, text = FALSE) +\n  geom_dag_point() +\n  geom_dag_text(color = \"red\") +\n  geom_dag_edges(edge_color = \"black\")"
  },
  {
    "objectID": "content/01_journal/06_rct.html#assignment-6.1",
    "href": "content/01_journal/06_rct.html#assignment-6.1",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "# Loading dataset into 'data'\ndata &lt;- readRDS(\"data/abtest_online.rds\")\nstr(data)\n\n#&gt; tibble [1,000 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ ip             : chr [1:1000] \"161.88.211.70\" \"239.86.201.0\" \"35.90.22.130\" \"219.176.193.3\" ...\n#&gt;  $ chatbot        : logi [1:1000] TRUE TRUE TRUE FALSE TRUE TRUE ...\n#&gt;  $ previous_visit : num [1:1000] 0 1 1 4 1 2 1 1 1 0 ...\n#&gt;  $ mobile_device  : logi [1:1000] FALSE FALSE FALSE TRUE FALSE FALSE ...\n#&gt;  $ purchase       : num [1:1000] 0 1 0 0 0 0 1 1 0 0 ...\n#&gt;  $ purchase_amount: num [1:1000] 0 39.5 0 0 0 ...\n\n# Convert non-numeric variables to numeric\ndata$mobile_device &lt;- as.numeric(data$mobile_device)\ndata$chatbot &lt;- as.numeric(data$chatbot)\n\n# Check covariate balance across groups using a plot \ncovariates &lt;- c(\"mobile_device\", \"previous_visit\", \"purchase\", \"purchase_amount\") # Variables of interest\ngroup_var &lt;- \"chatbot\"\n\n# Check covariate balance visually using plots\nplot_balances &lt;- lapply(covariates, function(var) {\n  ggplot(data, aes_string(x = group_var, y = var, color = as.factor(group_var))) +\n    stat_summary(\n      geom = \"errorbar\", \n      width = 0.5,\n      fun.data = \"mean_se\", \n      fun.args = list(mult = 1.96),\n      show.legend = FALSE\n    ) +\n    labs(title = paste(\"Covariate Balance for\", var))\n})\n\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\nplot_balances\n\n#&gt; [[1]]\n\n\n\n\n\n\n\n\n\n#&gt; \n#&gt; [[2]]\n\n\n\n\n\n\n\n\n\n#&gt; \n#&gt; [[3]]\n\n\n\n\n\n\n\n\n\n#&gt; \n#&gt; [[4]]\n\n\n\n\n\n\n\n\n\nPlots shows in general, covariates are overall balanced when chatbot is True and False. However, when comparing previous_visits plots it can be seen that the CI(Confidence Intervals) overlap with each other."
  },
  {
    "objectID": "content/01_journal/06_rct.html#assignment-6.2-running-a-regression-on-purchase_amount-to-find-the-effect-of-chatbot-on-sales",
    "href": "content/01_journal/06_rct.html#assignment-6.2-running-a-regression-on-purchase_amount-to-find-the-effect-of-chatbot-on-sales",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "sales_model &lt;- lm(purchase_amount ~ chatbot, data = data)\nsummary(sales_model) # regression summary\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbot      -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\nThe summary shows that the sales dropped for customers that used the chatbot with the chatbot coefficient value = -7.0756"
  },
  {
    "objectID": "content/01_journal/06_rct.html#assignment-6.3-interaction-between-chatbot-and-mobile_device-for-subgroup-effects",
    "href": "content/01_journal/06_rct.html#assignment-6.3-interaction-between-chatbot-and-mobile_device-for-subgroup-effects",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "interaction_model &lt;- lm(purchase_amount ~ chatbot * mobile_device, data = data)\nsummary(interaction_model) # regression summary (-0.1526 for chatbot:mobile_device)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -16.98 -14.54  -9.95  14.13  65.24 \n#&gt; \n#&gt; Coefficients:\n#&gt;                       Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            16.9797     1.0152  16.725   &lt;2e-16 ***\n#&gt; chatbot                -7.0301     1.4284  -4.922    1e-06 ***\n#&gt; mobile_device          -0.8727     1.7987  -0.485    0.628    \n#&gt; chatbot:mobile_device  -0.1526     2.5369  -0.060    0.952    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.66 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.03534,    Adjusted R-squared:  0.03244 \n#&gt; F-statistic: 12.16 on 3 and 996 DF,  p-value: 8.034e-08\n\n# Computing CATE for mobile users (subgroup)\nmobile_users &lt;- data[data$mobile_device == 1, ]\n\n# Fit a model for mobile users\nmobile_users_model &lt;- lm(purchase_amount ~ chatbot, data = mobile_users)\nsummary(mobile_users_model) # View regression summary\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = mobile_users)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.107 -16.107  -8.924  10.594  65.243 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   16.107      1.498  10.751  &lt; 2e-16 ***\n#&gt; chatbot       -7.183      2.115  -3.395 0.000773 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.83 on 315 degrees of freedom\n#&gt; Multiple R-squared:  0.03531,    Adjusted R-squared:  0.03225 \n#&gt; F-statistic: 11.53 on 1 and 315 DF,  p-value: 0.0007729"
  },
  {
    "objectID": "content/01_journal/06_rct.html#assignment-6.4-run-logistic-regression-for-binary-outcome-purchase",
    "href": "content/01_journal/06_rct.html#assignment-6.4-run-logistic-regression-for-binary-outcome-purchase",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "logistic_model &lt;- glm(purchase ~ chatbot, \n                      family = binomial(link = 'logit'), data = data)\nsummary(logistic_model) # View regression summary\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot, family = binomial(link = \"logit\"), \n#&gt;     data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.01613    0.08981  -0.180    0.857    \n#&gt; chatbot     -0.98939    0.13484  -7.337 2.18e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1273.3  on 998  degrees of freedom\n#&gt; AIC: 1277.3\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Now Interpreting the coefficient for 'chatbot'\n# Coefficients here represent log odds ratio. In order to get the odds ratio, coefficient for chatbot is exponentiated.\nexp(coef(logistic_model)[\"chatbot\"]) # Exponentiated coefficient for 'chatbot'\n\n#&gt;   chatbot \n#&gt; 0.3718025\n\n\nThe odds ratio amount to 0.371 which suggests that the customers are less likely to purchase something with the chatbot\n```"
  },
  {
    "objectID": "content/01_journal/07_matching.html#assignment-7.1",
    "href": "content/01_journal/07_matching.html#assignment-7.1",
    "title": "Matching and Subclassification",
    "section": "1 Assignment 7.1",
    "text": "1 Assignment 7.1\n\n# Load the data file membership.rds\ndata &lt;- readRDS(\"data/membership.rds\")\n\n# Explore the structure and summary of the data\nstr(data)\n\n#&gt; tibble [10,000 × 5] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ age          : num [1:10000] 31.3 40.7 23.2 65.3 27.2 52.4 39.5 26.5 28.4 34.7 ...\n#&gt;  $ sex          : int [1:10000] 1 1 1 0 0 1 0 1 1 1 ...\n#&gt;  $ pre_avg_purch: num [1:10000] 61.1 43 18 82 61.4 ...\n#&gt;  $ card         : int [1:10000] 0 1 1 1 0 0 1 0 0 1 ...\n#&gt;  $ avg_purch    : num [1:10000] 70.8 51.4 26 124 45.1 ...\n\nsummary(data)\n\n#&gt;       age             sex         pre_avg_purch         card       \n#&gt;  Min.   :16.00   Min.   :0.0000   Min.   :-14.23   Min.   :0.0000  \n#&gt;  1st Qu.:29.80   1st Qu.:0.0000   1st Qu.: 51.82   1st Qu.:0.0000  \n#&gt;  Median :38.80   Median :1.0000   Median : 70.15   Median :0.0000  \n#&gt;  Mean   :40.37   Mean   :0.5038   Mean   : 70.42   Mean   :0.4232  \n#&gt;  3rd Qu.:49.20   3rd Qu.:1.0000   3rd Qu.: 88.79   3rd Qu.:1.0000  \n#&gt;  Max.   :90.00   Max.   :1.0000   Max.   :169.42   Max.   :1.0000  \n#&gt;    avg_purch     \n#&gt;  Min.   :-28.61  \n#&gt;  1st Qu.: 54.02  \n#&gt;  Median : 76.24  \n#&gt;  Mean   : 76.61  \n#&gt;  3rd Qu.: 98.54  \n#&gt;  Max.   :192.91\n\n# Compute the correlation and correlation matrix\ncorrelations &lt;- cor(data)\ncorrelations\n\n#&gt;                      age          sex pre_avg_purch        card   avg_purch\n#&gt; age           1.00000000  0.012532675   0.517506430 0.105533628 0.448632638\n#&gt; sex           0.01253267  1.000000000  -0.001221386 0.008468092 0.002181853\n#&gt; pre_avg_purch 0.51750643 -0.001221386   1.000000000 0.192333327 0.855828507\n#&gt; card          0.10553363  0.008468092   0.192333327 1.000000000 0.382352233\n#&gt; avg_purch     0.44863264  0.002181853   0.855828507 0.382352233 1.000000000\n\ncorrelation_matrix &lt;- cor(data)\n\n# Plot the correlation matrix as a heatmap\nheatmap(correlation_matrix, \n        Colv = NA, Rowv = NA,\n        col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(100),\n        cexRow = 0.8, cexCol = 0.8,  # Adjust label size\n        las = 2)\n\n\n\n\n\n\n\nggpairs(data, \n        columns = 1:4,  # Select specific columns if needed\n        lower = list(continuous = \"points\", aes(color = card)),  # Customize aesthetics for lower cells\n        diag = list(continuous = \"barDiag\"))  # Customize diagonal cells\n\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nStrong Positive Correlations:\npre_avg_purch and avg_purch have a strong positive correlation of 0.8558. age and pre_avg_purch exhibit a relatively strong positive correlation of 0.5175. avg_purch and age show a moderate positive correlation of 0.4486.\n\n\nModerate Correlations:\ncard and avg_purch have a moderate positive correlation of 0.3824. card and age show a weaker positive correlation of 0.1055.\n\n\nWeak Correlations:\nsex doesn’t exhibit significant correlations with other variables (all correlations are close to zero).\n\n# Define the DAG based on above observation\nmembership_dag &lt;- dagify(\n  avg_purch ~ pre_avg_purch,\n  card ~ age,\n  avg_purch ~ card,\n  pre_avg_purch ~ age,\n  avg_purch ~ age,\n  coords = list(x = c(avg_purch = 1, pre_avg_purch = 2, card = 3, age = 4),\n                y = c(avg_purch = 0, pre_avg_purch = 1, card = 0, age = 1))\n)\n\n# Plot the DAG \nggdag(membership_dag, text = FALSE) +\n  geom_dag_point() +\n  geom_dag_text(color = \"red\") +\n  geom_dag_edges(edge_color = \"black\")"
  },
  {
    "objectID": "content/01_journal/07_matching.html#assignment-7.2",
    "href": "content/01_journal/07_matching.html#assignment-7.2",
    "title": "Matching and Subclassification",
    "section": "2 Assignment 7.2",
    "text": "2 Assignment 7.2\n\ntreatment_group &lt;- subset(data, card == 1)  \ncontrol_group &lt;- subset(data, card == 0)    \n\n# Calculate Average Outcome for Each Group\navg_outcome_treatment &lt;- mean(treatment_group$avg_purch)  \navg_outcome_control &lt;- mean(control_group$avg_purch)     \n\n# Compute ATE\nATE &lt;- avg_outcome_treatment - avg_outcome_control\n\n# Print the ATE\nprint(paste(\"Average Treatment Effect (ATE):\", ATE))\n\n#&gt; [1] \"Average Treatment Effect (ATE): 25.2195043546585\""
  },
  {
    "objectID": "content/01_journal/07_matching.html#assignment-7.3.1",
    "href": "content/01_journal/07_matching.html#assignment-7.3.1",
    "title": "Matching and Subclassification",
    "section": "3 Assignment 7.3.1",
    "text": "3 Assignment 7.3.1\n\n# (Coarsened) Exact Matching\n# Without specifying coarsening\n# (1) Matching\ncem &lt;- matchit(card ~ age + pre_avg_purch,\n               data = data, \n               method = 'cem', \n               estimand = 'ATE')\n\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch, data = data, method = \"cem\", \n#&gt;     estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.2869       40.2645          0.0017     1.0001    0.0016\n#&gt; pre_avg_purch       70.5402       70.1875          0.0135     0.9910    0.0044\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0067          0.1224\n#&gt; pre_avg_purch   0.0133          0.1557\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.   4232.  \n#&gt; Matched (ESS) 5527.11 3928.04\n#&gt; Matched       5752.   4199.  \n#&gt; Unmatched       16.     33.  \n#&gt; Discarded        0.      0.\n\n# Use matched data\ndata_cem &lt;- match.data(cem)\n\n# (2) Estimation\nmodel_cem &lt;- lm(avg_purch ~ card, data = data_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = data_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -155.31  -20.74   -0.17   20.25  146.91 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.0474     0.3996  175.30   &lt;2e-16 ***\n#&gt; card         15.2687     0.6151   24.82   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.31 on 9949 degrees of freedom\n#&gt; Multiple R-squared:  0.05831,    Adjusted R-squared:  0.05822 \n#&gt; F-statistic: 616.1 on 1 and 9949 DF,  p-value: &lt; 2.2e-16\n\n\nThe precise estimate using (Coarsened) Exact Matching for Card was found to be 15.2687"
  },
  {
    "objectID": "content/01_journal/07_matching.html#assignment-7.3.2",
    "href": "content/01_journal/07_matching.html#assignment-7.3.2",
    "title": "Matching and Subclassification",
    "section": "4 Assignment 7.3.2",
    "text": "4 Assignment 7.3.2\n\n# NN matching\n# replace: one-to-one or one-to-many matching\n# (1) Matching\nnn &lt;- matchit(card ~ age + pre_avg_purch,\n              data = data,\n              method = \"nearest\",\n              distance = \"mahalanobis\", \n              replace = TRUE)\n\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch, data = data, method = \"nearest\", \n#&gt;     distance = \"mahalanobis\", replace = TRUE)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       42.0132          0.0014     1.0078    0.0011\n#&gt; pre_avg_purch       76.3938       76.3390          0.0021     1.0075    0.0011\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0045          0.0202\n#&gt; pre_avg_purch   0.0057          0.0219\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 2032.44    4232\n#&gt; Matched       2700.      4232\n#&gt; Unmatched     3068.         0\n#&gt; Discarded        0.         0\n\n# Use matched data\ndata_nn &lt;- match.data(nn)\n\n# (2) Estimation\nmodel_nn &lt;- lm(avg_purch ~ card, data = data_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = data_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -21.159   -1.517   18.610  181.319 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.2067     0.5844  130.40   &lt;2e-16 ***\n#&gt; card         14.9524     0.7479   19.99   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.37 on 6930 degrees of freedom\n#&gt; Multiple R-squared:  0.05453,    Adjusted R-squared:  0.05439 \n#&gt; F-statistic: 399.7 on 1 and 6930 DF,  p-value: &lt; 2.2e-16\n\n\nThe precise estimate using Nearest Neighbor Matching for Card was found to be 14.9524"
  },
  {
    "objectID": "content/01_journal/07_matching.html#assignment-7.3.3",
    "href": "content/01_journal/07_matching.html#assignment-7.3.3",
    "title": "Matching and Subclassification",
    "section": "5 Assignment 7.3.3",
    "text": "5 Assignment 7.3.3\n\n#Inverse Probability Weighting\n# (1) Propensity scores\nmodel_prop &lt;- glm(card ~ age + pre_avg_purch,\n                  data = data,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4121735  0.0723729 -19.512   &lt;2e-16 ***\n#&gt; age            0.0011734  0.0017758   0.661    0.509    \n#&gt; pre_avg_purch  0.0148181  0.0009263  15.996   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13250  on 9997  degrees of freedom\n#&gt; AIC: 13256\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Add propensities to table\ndf_aug &lt;- data %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\n# Look at data with IPW scores\nselected_columns &lt;- df_ipw[c(\"age\", \"pre_avg_purch\", \"card\", \"propensity\", \"ipw\")]\nselected_columns\n\n\n\n  \n\n\n# (2) Estimation\nmodel_ipw &lt;- lm(avg_purch ~ card,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -203.886  -29.009   -0.273   28.782  215.682 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2627     0.4320  162.65   &lt;2e-16 ***\n#&gt; card         14.9548     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.2 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05654,    Adjusted R-squared:  0.05645 \n#&gt; F-statistic: 599.2 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n# Plot histogram of estimated propensities\nggplot(df_aug, aes(x = propensity)) +\n  geom_histogram(alpha = .8, color = \"white\")\n\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# Looking for observations with highest weights\ndf_ipw %&gt;% mutate(card, age, pre_avg_purch, propensity, ipw) %&gt;% \n  arrange(desc(ipw))\n\n\n\n  \n\n\n# Run with high weights excluded\nmodel_ipw_trim &lt;- lm(avg_purch ~ card,\n                data = df_ipw %&gt;% filter(propensity %&gt;% between(0.15, 0.85)),\n                weights = ipw)\nsummary(model_ipw_trim)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw %&gt;% filter(propensity %&gt;% \n#&gt;     between(0.15, 0.85)), weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -203.886  -29.009   -0.273   28.782  215.682 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2627     0.4320  162.65   &lt;2e-16 ***\n#&gt; card         14.9548     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.2 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05654,    Adjusted R-squared:  0.05645 \n#&gt; F-statistic: 599.2 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\nThe precise estimate using Inverse Probability Weighting for Card was found to be 14.9548"
  },
  {
    "objectID": "content/01_journal/08_did.html#assignment-8.1",
    "href": "content/01_journal/08_did.html#assignment-8.1",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "library(Matching)\nlibrary(ggdag)\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(MatchIt)\n\n# loading data into data\ndata &lt;- readRDS(\"data/hospdd.rds\")\ndata\n\n\n\n  \n\n\nnames(data)\n\n#&gt; [1] \"hospital\"  \"frequency\" \"month\"     \"procedure\" \"satis\"\n\nmean_before_treatment &lt;- data %&gt;%\n  filter(procedure == 0, hospital&gt;= 1 & hospital&lt;=18) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_after_treatment &lt;- data %&gt;%\n  filter(procedure == 1, hospital&gt;= 1 & hospital&lt;=18) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_before_control &lt;- data %&gt;%\n  filter(procedure == 0, hospital&gt;18 & hospital&lt;= 46) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_after_control &lt;- data %&gt;%\n  filter(procedure == 0, hospital&gt;18 & hospital&lt;= 46) %&gt;%\n  pull(satis) %&gt;%\n  mean()\ndiff_before &lt;- mean_before_treatment - mean_before_control\ndiff_after &lt;- mean_after_treatment - mean_after_control\n\ndiff_in_diff &lt;- diff_after - diff_before\n\nsprintf(\"Mean satisfaction for treated hospitals before treatment: %f\" ,mean_before_treatment)\n\n#&gt; [1] \"Mean satisfaction for treated hospitals before treatment: 3.525383\"\n\nsprintf(\"Mean satisfaction for treated hospitals after treatment: %f\", mean_after_treatment)\n\n#&gt; [1] \"Mean satisfaction for treated hospitals after treatment: 4.363351\"\n\nsprintf(\"Mean satisfaction for control hospitals before treatment: %f\" , mean_before_control)\n\n#&gt; [1] \"Mean satisfaction for control hospitals before treatment: 3.387499\"\n\nsprintf(\"Mean satisfaction for control hospitals after treatment: %f\", mean_after_control)\n\n#&gt; [1] \"Mean satisfaction for control hospitals after treatment: 3.387499\"\n\nsprintf(\"Difference in difference: %f\", diff_in_diff)\n\n#&gt; [1] \"Difference in difference: 0.837968\""
  },
  {
    "objectID": "content/01_journal/08_did.html#assignment-8.2",
    "href": "content/01_journal/08_did.html#assignment-8.2",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "# Linear regression with month and hospital as continuous variables\nmodel_continuous &lt;- lm(satis ~ month + hospital + procedure, data = data)\nsummary(model_continuous)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ month + hospital + procedure, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.3126 -0.6548 -0.0933  0.5555  5.3347 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  3.538024   0.031365 112.801  &lt; 2e-16 ***\n#&gt; month       -0.004965   0.006392  -0.777 0.437378    \n#&gt; hospital    -0.003731   0.001043  -3.576 0.000351 ***\n#&gt; procedure    0.886120   0.039143  22.638  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9836 on 7364 degrees of freedom\n#&gt; Multiple R-squared:  0.1325, Adjusted R-squared:  0.1321 \n#&gt; F-statistic: 374.8 on 3 and 7364 DF,  p-value: &lt; 2.2e-16\n\n# Linear regression with factors for month and hospital\nmodel_factors &lt;- lm(satis ~ as.factor(month) + as.factor(hospital) + procedure, data = data)\nsummary(model_factors)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(month) + as.factor(hospital) + \n#&gt;     procedure, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.1880 -0.4644  0.0067  0.4539  4.2921 \n#&gt; \n#&gt; Coefficients:\n#&gt;                         Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            3.1716566  0.0562207  56.414  &lt; 2e-16 ***\n#&gt; as.factor(month)2     -0.0096077  0.0292119  -0.329 0.742244    \n#&gt; as.factor(month)3      0.0219686  0.0292119   0.752 0.452050    \n#&gt; as.factor(month)4     -0.0032839  0.0324936  -0.101 0.919504    \n#&gt; as.factor(month)5     -0.0094027  0.0324936  -0.289 0.772305    \n#&gt; as.factor(month)6     -0.0038375  0.0324936  -0.118 0.905990    \n#&gt; as.factor(month)7     -0.0111941  0.0324936  -0.345 0.730478    \n#&gt; as.factor(hospital)2   0.4085664  0.0772418   5.289 1.26e-07 ***\n#&gt; as.factor(hospital)3   0.5336248  0.0793384   6.726 1.88e-11 ***\n#&gt; as.factor(hospital)4   0.2275102  0.0739411   3.077 0.002099 ** \n#&gt; as.factor(hospital)5  -0.1453529  0.0739411  -1.966 0.049360 *  \n#&gt; as.factor(hospital)6   0.4478634  0.0739411   6.057 1.46e-09 ***\n#&gt; as.factor(hospital)7   1.4044164  0.0714559  19.654  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8   0.0718758  0.0763186   0.942 0.346333    \n#&gt; as.factor(hospital)9  -1.5185150  0.0782447 -19.407  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10  1.6828446  0.0772418  21.787  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11  0.2209653  0.0763186   2.895 0.003799 ** \n#&gt; as.factor(hospital)12 -0.0953034  0.0782447  -1.218 0.223256    \n#&gt; as.factor(hospital)13  0.4955931  0.0754658   6.567 5.48e-11 ***\n#&gt; as.factor(hospital)14  0.2330426  0.0793384   2.937 0.003321 ** \n#&gt; as.factor(hospital)15 -0.1444935  0.0793384  -1.821 0.068613 .  \n#&gt; as.factor(hospital)16  1.4142680  0.0772418  18.310  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17  0.4235429  0.0805362   5.259 1.49e-07 ***\n#&gt; as.factor(hospital)18  0.1532761  0.0938164   1.634 0.102346    \n#&gt; as.factor(hospital)19 -0.7453017  0.0811623  -9.183  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20  0.0473874  0.0791140   0.599 0.549207    \n#&gt; as.factor(hospital)21  1.1943370  0.0836232  14.282  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22  0.7993153  0.0823336   9.708  &lt; 2e-16 ***\n#&gt; as.factor(hospital)23  0.7017202  0.0811623   8.646  &lt; 2e-16 ***\n#&gt; as.factor(hospital)24 -0.3081260  0.0866402  -3.556 0.000378 ***\n#&gt; as.factor(hospital)25  0.6464736  0.0927258   6.972 3.40e-12 ***\n#&gt; as.factor(hospital)26  0.2142471  0.0791140   2.708 0.006783 ** \n#&gt; as.factor(hospital)27 -0.3986544  0.0766106  -5.204 2.01e-07 ***\n#&gt; as.factor(hospital)28  0.7119953  0.0836232   8.514  &lt; 2e-16 ***\n#&gt; as.factor(hospital)29  0.2485512  0.0800935   3.103 0.001921 ** \n#&gt; as.factor(hospital)30 -0.1679220  0.0953638  -1.761 0.078304 .  \n#&gt; as.factor(hospital)31  0.5120848  0.0791140   6.473 1.02e-10 ***\n#&gt; as.factor(hospital)32 -0.3233456  0.0800935  -4.037 5.47e-05 ***\n#&gt; as.factor(hospital)33 -0.4539752  0.0791140  -5.738 9.95e-09 ***\n#&gt; as.factor(hospital)34 -0.0004123  0.0746054  -0.006 0.995590    \n#&gt; as.factor(hospital)35  0.3541110  0.0766106   4.622 3.86e-06 ***\n#&gt; as.factor(hospital)36  2.1381425  0.0773811  27.631  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37  0.1404036  0.0927258   1.514 0.130023    \n#&gt; as.factor(hospital)38 -0.0868060  0.0782129  -1.110 0.267093    \n#&gt; as.factor(hospital)39 -0.0234969  0.0823336  -0.285 0.775356    \n#&gt; as.factor(hospital)40  1.1215331  0.0782129  14.339  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41 -0.1497346  0.0766106  -1.954 0.050681 .  \n#&gt; as.factor(hospital)42  0.8811369  0.0850508  10.360  &lt; 2e-16 ***\n#&gt; as.factor(hospital)43 -0.7724325  0.0811623  -9.517  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44  0.0344120  0.0904337   0.381 0.703569    \n#&gt; as.factor(hospital)45 -0.2137495  0.0766106  -2.790 0.005283 ** \n#&gt; as.factor(hospital)46  0.0784915  0.0823336   0.953 0.340452    \n#&gt; procedure              0.8479879  0.0342191  24.781  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7238 on 7315 degrees of freedom\n#&gt; Multiple R-squared:  0.5333, Adjusted R-squared:  0.5299 \n#&gt; F-statistic: 160.7 on 52 and 7315 DF,  p-value: &lt; 2.2e-16\n\n\nThe model using categorical variables for ‘month’ and ‘hospital’ demonstrates a better fit explaining 52.99% of the variance in patient satisfaction and improved explanatory power compared to the continuous variable model. Individual hospitals and certain months have significant impacts on patient satisfaction besides the effect of the procedure."
  },
  {
    "objectID": "content/01_journal/09_iv.html#assignment-9.1",
    "href": "content/01_journal/09_iv.html#assignment-9.1",
    "title": "Instrumental Variables",
    "section": "",
    "text": "library(Matching)\n\n#&gt; Loading required package: MASS\n\n\n#&gt; ## \n#&gt; ##  Matching (Version 4.10-14, Build Date: 2023-09-13)\n#&gt; ##  See https://www.jsekhon.com for additional documentation.\n#&gt; ##  Please cite software as:\n#&gt; ##   Jasjeet S. Sekhon. 2011. ``Multivariate and Propensity Score Matching\n#&gt; ##   Software with Automated Balance Optimization: The Matching package for R.''\n#&gt; ##   Journal of Statistical Software, 42(7): 1-52. \n#&gt; ##\n\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2\n\n\n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks ggdag::filter(), stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ✖ dplyr::select() masks MASS::select()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(MatchIt)\n\n\ndata &lt;- readRDS(\"data/rand_enc.rds\")\ndata"
  },
  {
    "objectID": "content/01_journal/01_probability.html#given-the-following-definitions-1",
    "href": "content/01_journal/01_probability.html#given-the-following-definitions-1",
    "title": "Probability Theory",
    "section": "3.1 Given the following definitions:",
    "text": "3.1 Given the following definitions:\nA: Product is faulty vs !A: Product is flawless\nB: Alarm is triggered vs !B: No Alarm\nP(A): Probability of product being faulty\nP(B∣A): Probability of alarm being triggered when product is faulty.\nP(B∣!A): Probability of alarm being triggered when product is flawless.\n\nP_A &lt;- 0.04\nP_not_A &lt;- 1 - P_A\nP_B_given_A &lt;- 0.97\nP_B_given_not_A &lt;- 0.01\n\n\n# For P(!A|B) and P(A|B) we use Bayes' theorem but for that we need to find P(B) first which is the probability of alarm being triggered:\n\nP_B &lt;- (P_B_given_A*P_A) + (P_B_given_not_A*P_not_A)\n\nP_not_A_given_B &lt;- (P_B_given_not_A*P_not_A)/P_B\n\nP_not_A_given_B\n\n#&gt; [1] 0.1983471\n\nP_A_given_B &lt;- (P_B_given_A*P_A)/P_B\n\nP_A_given_B\n\n#&gt; [1] 0.8016529\n\n\nThese results show that in case the alarm is triggered, there is a possibility of about 19.83 % that the product is flawless and a probability of 80.16 % that the product is faulty."
  },
  {
    "objectID": "content/01_journal/01_probability.html#these-results-show-that-in-case-the-alarm-is-triggered-there-is-a-possibility-of-about-19.83-that-the-product-is-flawless-and-a-probability-of-80.16-that-the-product-is-faulty.",
    "href": "content/01_journal/01_probability.html#these-results-show-that-in-case-the-alarm-is-triggered-there-is-a-possibility-of-about-19.83-that-the-product-is-flawless-and-a-probability-of-80.16-that-the-product-is-faulty.",
    "title": "Probability Theory",
    "section": "3.2 These results show that in case the alarm is triggered, there is a possibility of about 19.83 % that the product is flawless and a probability of 80.16 % that the product is faulty.",
    "text": "3.2 These results show that in case the alarm is triggered, there is a possibility of about 19.83 % that the product is flawless and a probability of 80.16 % that the product is faulty."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#assignment-3.1",
    "href": "content/01_journal/02_statistics.html#assignment-3.1",
    "title": "Statistical Concepts",
    "section": "",
    "text": "#load libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n# Load the data from random_vars.rds file\ndata &lt;- readRDS(\"data/random_vars.rds\")  # Replace 'path_to_your_folder' with the actual path\n\n# View the loaded data\nView(data)\n\n# Compute expected value, variance, and standard deviation for each variable\nsummary_stats &lt;- data.frame(\n  Variable = colnames(data),\n  Expected_Value = sapply(data, mean),\n  Variance = sapply(data, var),\n  Standard_Deviation = sapply(data, sd)\n)\n\n# View summary statistics\nsummary_stats"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#assignment-3.2",
    "href": "content/01_journal/02_statistics.html#assignment-3.2",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Standard deviation provides insights into the variability of individual variables, comparing relationships between ‘age’ and ‘income’ would benefit more from measures like covariance and correlation to understand how these variables are related or associated with each other but comparing covariance is not easy. Where as in correlation, we get values between -1 and 1 which provides a standardized measure to interpret the strength and direction of linear relationship between age and income."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#assignment-3.3",
    "href": "content/01_journal/02_statistics.html#assignment-3.3",
    "title": "Statistical Concepts",
    "section": "",
    "text": "# Compute expected value, variance, and standard deviation for each variable\nsummary_stats &lt;- data.frame(\n  Variable = colnames(data),\n  Covariance = sapply(data, function(x) sapply(data, function(y) cov(x, y))),\n  Correlation = sapply(data, function(x) sapply(data, function(y) cor(x, y)))\n)\n\n# View summary statistics\nsummary_stats\n\n\n\n  \n\n\n\nIt is easier to interpret correlation as compared to covarience as discussed above."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#assignment-3.4",
    "href": "content/01_journal/02_statistics.html#assignment-3.4",
    "title": "Statistical Concepts",
    "section": "",
    "text": "# Calculate conditional expected values for income based on age ranges\nconditional_expected_value &lt;- data %&gt;%\n  summarize(\n    E_income_age_18 = mean(filter(., age &lt;= 18)$income),\n    E_income_age_18_65 = mean(filter(., age &gt;= 18 & age &lt; 65)$income),\n    E_income_age_65 = mean(filter(., age &gt;= 65)$income)\n  )\n\nconditional_expected_value"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#assignment-2.1",
    "href": "content/01_journal/02_statistics.html#assignment-2.1",
    "title": "Statistical Concepts",
    "section": "",
    "text": "#load libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n# Load the data from random_vars.rds file\ndata &lt;- readRDS(\"data/random_vars.rds\")  # Replace 'path_to_your_folder' with the actual path\n\n# View the loaded data\nView(data)\n\n# Compute expected value, variance, and standard deviation for each variable\nsummary_stats &lt;- data.frame(\n  Variable = colnames(data),\n  Expected_Value = sapply(data, mean),\n  Variance = sapply(data, var),\n  Standard_Deviation = sapply(data, sd)\n)\n\n# View summary statistics\nsummary_stats"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#assignment-2.2",
    "href": "content/01_journal/02_statistics.html#assignment-2.2",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Standard deviation provides insights into the variability of individual variables, comparing relationships between ‘age’ and ‘income’ would benefit more from measures like covariance and correlation to understand how these variables are related or associated with each other but comparing covariance is not easy. Where as in correlation, we get values between -1 and 1 which provides a standardized measure to interpret the strength and direction of linear relationship between age and income."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#assignment-2.3",
    "href": "content/01_journal/02_statistics.html#assignment-2.3",
    "title": "Statistical Concepts",
    "section": "",
    "text": "# Compute expected value, variance, and standard deviation for each variable\nsummary_stats &lt;- data.frame(\n  Variable = colnames(data),\n  Covariance = sapply(data, function(x) sapply(data, function(y) cov(x, y))),\n  Correlation = sapply(data, function(x) sapply(data, function(y) cor(x, y)))\n)\n\n# View summary statistics\nsummary_stats\n\n\n\n  \n\n\n\nIt is easier to interpret correlation as compared to covarience as discussed above."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#assignment-2.4",
    "href": "content/01_journal/02_statistics.html#assignment-2.4",
    "title": "Statistical Concepts",
    "section": "",
    "text": "# Calculate conditional expected values for income based on age ranges\nconditional_expected_value &lt;- data %&gt;%\n  summarize(\n    E_income_age_18 = mean(filter(., age &lt;= 18)$income),\n    E_income_age_18_65 = mean(filter(., age &gt;= 18 & age &lt; 65)$income),\n    E_income_age_65 = mean(filter(., age &gt;= 65)$income)\n  )\n\nconditional_expected_value"
  },
  {
    "objectID": "content/01_journal/03_regression.html#assignment-3.1",
    "href": "content/01_journal/03_regression.html#assignment-3.1",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "#load libraries\nlibrary(dplyr)\nlibrary(ggplot2)\n\n#Read the data and check dimensions\ndata &lt;- readRDS(\"data/car_prices.rds\")\ndim(data) # Check dimensions - rows and columns\n\n#&gt; [1] 181  22"
  },
  {
    "objectID": "content/01_journal/03_regression.html#assignment-3.2",
    "href": "content/01_journal/03_regression.html#assignment-3.2",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "#Get a more detailed look at the data\nstr(data) # Display structure and data types\n\n#&gt; tibble [181 × 22] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ aspiration      : chr [1:181] \"std\" \"std\" \"std\" \"std\" ...\n#&gt;  $ doornumber      : chr [1:181] \"two\" \"two\" \"two\" \"four\" ...\n#&gt;  $ carbody         : chr [1:181] \"convertible\" \"convertible\" \"hatchback\" \"sedan\" ...\n#&gt;  $ drivewheel      : chr [1:181] \"rwd\" \"rwd\" \"rwd\" \"fwd\" ...\n#&gt;  $ enginelocation  : chr [1:181] \"front\" \"front\" \"front\" \"front\" ...\n#&gt;  $ wheelbase       : num [1:181] 88.6 88.6 94.5 99.8 99.4 ...\n#&gt;  $ carlength       : num [1:181] 169 169 171 177 177 ...\n#&gt;  $ carwidth        : num [1:181] 64.1 64.1 65.5 66.2 66.4 66.3 71.4 71.4 71.4 67.9 ...\n#&gt;  $ carheight       : num [1:181] 48.8 48.8 52.4 54.3 54.3 53.1 55.7 55.7 55.9 52 ...\n#&gt;  $ curbweight      : num [1:181] 2548 2548 2823 2337 2824 ...\n#&gt;  $ enginetype      : chr [1:181] \"dohc\" \"dohc\" \"ohcv\" \"ohc\" ...\n#&gt;  $ cylindernumber  : chr [1:181] \"four\" \"four\" \"six\" \"four\" ...\n#&gt;  $ enginesize      : num [1:181] 130 130 152 109 136 136 136 136 131 131 ...\n#&gt;  $ fuelsystem      : chr [1:181] \"mpfi\" \"mpfi\" \"mpfi\" \"mpfi\" ...\n#&gt;  $ boreratio       : num [1:181] 3.47 3.47 2.68 3.19 3.19 3.19 3.19 3.19 3.13 3.13 ...\n#&gt;  $ stroke          : num [1:181] 2.68 2.68 3.47 3.4 3.4 3.4 3.4 3.4 3.4 3.4 ...\n#&gt;  $ compressionratio: num [1:181] 9 9 9 10 8 8.5 8.5 8.5 8.3 7 ...\n#&gt;  $ horsepower      : num [1:181] 111 111 154 102 115 110 110 110 140 160 ...\n#&gt;  $ peakrpm         : num [1:181] 5000 5000 5000 5500 5500 5500 5500 5500 5500 5500 ...\n#&gt;  $ citympg         : num [1:181] 21 21 19 24 18 19 19 19 17 16 ...\n#&gt;  $ highwaympg      : num [1:181] 27 27 26 30 22 25 25 25 20 22 ...\n#&gt;  $ price           : num [1:181] 13495 16500 16500 13950 17450 ...\n\nglimpse(data)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\n\nFor strings  Character and for numbers  Double data types are observed"
  },
  {
    "objectID": "content/01_journal/03_regression.html#assignment-3.3",
    "href": "content/01_journal/03_regression.html#assignment-3.3",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "#We Run a linear regression \nmodel &lt;- lm(price ~ ., data = data[, -20])\nsummary(model) # Check regression summary for coefficients, p-values, etc.\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = data[, -20])\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -5715.4 -1192.9   -27.4   828.6  8978.4 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -3.553e+04  1.536e+04  -2.313 0.022149 *  \n#&gt; aspirationturbo       1.797e+03  1.035e+03   1.736 0.084626 .  \n#&gt; doornumbertwo         2.242e+02  5.695e+02   0.394 0.694471    \n#&gt; carbodyhardtop       -3.676e+03  1.421e+03  -2.587 0.010669 *  \n#&gt; carbodyhatchback     -3.338e+03  1.235e+03  -2.702 0.007710 ** \n#&gt; carbodysedan         -2.296e+03  1.353e+03  -1.697 0.091801 .  \n#&gt; carbodywagon         -3.444e+03  1.486e+03  -2.317 0.021887 *  \n#&gt; drivewheelfwd        -4.314e+02  1.066e+03  -0.405 0.686166    \n#&gt; drivewheelrwd        -1.558e+01  1.265e+03  -0.012 0.990190    \n#&gt; enginelocationrear    6.858e+03  2.535e+03   2.705 0.007659 ** \n#&gt; wheelbase            -3.977e+01  9.086e+01  -0.438 0.662247    \n#&gt; carlength            -2.392e+01  5.042e+01  -0.474 0.635924    \n#&gt; carwidth              7.244e+02  2.436e+02   2.974 0.003444 ** \n#&gt; carheight             1.163e+02  1.337e+02   0.870 0.385828    \n#&gt; curbweight            2.621e+00  1.776e+00   1.475 0.142293    \n#&gt; enginetypedohcv      -7.913e+03  4.595e+03  -1.722 0.087168 .  \n#&gt; enginetypel           1.101e+03  1.768e+03   0.623 0.534398    \n#&gt; enginetypeohc         3.440e+03  9.141e+02   3.764 0.000243 ***\n#&gt; enginetypeohcf        8.991e+02  1.616e+03   0.556 0.578818    \n#&gt; enginetypeohcv       -6.170e+03  1.230e+03  -5.018 1.52e-06 ***\n#&gt; cylindernumberfive   -1.154e+04  2.992e+03  -3.856 0.000173 ***\n#&gt; cylindernumberfour   -1.135e+04  3.147e+03  -3.605 0.000429 ***\n#&gt; cylindernumbersix    -6.918e+03  2.200e+03  -3.144 0.002023 ** \n#&gt; cylindernumberthree  -4.249e+03  4.676e+03  -0.909 0.365015    \n#&gt; cylindernumbertwelve -1.101e+04  4.181e+03  -2.633 0.009386 ** \n#&gt; enginesize            1.257e+02  2.647e+01   4.749 4.89e-06 ***\n#&gt; fuelsystem2bbl        2.203e+02  8.779e+02   0.251 0.802254    \n#&gt; fuelsystemmfi        -3.116e+03  2.567e+03  -1.214 0.226855    \n#&gt; fuelsystemmpfi        4.302e+02  9.905e+02   0.434 0.664720    \n#&gt; fuelsystemspdi       -2.523e+03  1.360e+03  -1.856 0.065553 .  \n#&gt; fuelsystemspfi        5.316e+02  2.493e+03   0.213 0.831430    \n#&gt; boreratio            -1.219e+03  1.630e+03  -0.748 0.455731    \n#&gt; stroke               -4.583e+03  9.148e+02  -5.009 1.58e-06 ***\n#&gt; compressionratio     -7.785e+02  5.495e+02  -1.417 0.158716    \n#&gt; horsepower            1.139e+01  2.256e+01   0.505 0.614476    \n#&gt; peakrpm               2.522e+00  6.325e-01   3.988 0.000105 ***\n#&gt; highwaympg            7.601e+01  8.220e+01   0.925 0.356679    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2184 on 144 degrees of freedom\n#&gt; Multiple R-squared:  0.9414, Adjusted R-squared:  0.9267 \n#&gt; F-statistic: 64.24 on 36 and 144 DF,  p-value: &lt; 2.2e-16\n\n\nWe want to now consider seeing p-values for selecting significant regressors affecting price and we construct a barplot with alpha set to 0.05 and shown with red dotted line. All those variables with p-value greater than 0.05 we don’t consider them.\n\n#Extracting p-values of regressors from the summary of the regression model\np_values &lt;- summary(model)$coefficients[, \"Pr(&gt;|t|)\"]\np_values\n\n#&gt;          (Intercept)      aspirationturbo        doornumbertwo \n#&gt;         2.214873e-02         8.462557e-02         6.944711e-01 \n#&gt;       carbodyhardtop     carbodyhatchback         carbodysedan \n#&gt;         1.066928e-02         7.710289e-03         9.180131e-02 \n#&gt;         carbodywagon        drivewheelfwd        drivewheelrwd \n#&gt;         2.188745e-02         6.861656e-01         9.901899e-01 \n#&gt;   enginelocationrear            wheelbase            carlength \n#&gt;         7.658904e-03         6.622471e-01         6.359238e-01 \n#&gt;             carwidth            carheight           curbweight \n#&gt;         3.443999e-03         3.858277e-01         1.422933e-01 \n#&gt;      enginetypedohcv          enginetypel        enginetypeohc \n#&gt;         8.716763e-02         5.343980e-01         2.431410e-04 \n#&gt;       enginetypeohcf       enginetypeohcv   cylindernumberfive \n#&gt;         5.788177e-01         1.518253e-06         1.731842e-04 \n#&gt;   cylindernumberfour    cylindernumbersix  cylindernumberthree \n#&gt;         4.293199e-04         2.022597e-03         3.650153e-01 \n#&gt; cylindernumbertwelve           enginesize       fuelsystem2bbl \n#&gt;         9.386403e-03         4.887489e-06         8.022540e-01 \n#&gt;        fuelsystemmfi       fuelsystemmpfi       fuelsystemspdi \n#&gt;         2.268549e-01         6.647200e-01         6.555294e-02 \n#&gt;       fuelsystemspfi            boreratio               stroke \n#&gt;         8.314303e-01         4.557306e-01         1.576896e-06 \n#&gt;     compressionratio           horsepower              peakrpm \n#&gt;         1.587161e-01         6.144755e-01         1.054750e-04 \n#&gt;           highwaympg \n#&gt;         3.566786e-01\n\n# Extracting names of the regressors\nregressor_names &lt;- names(p_values)\n\n# Create a bar plot to visualize the significance of regressors\nbarplot(-log10(p_values), \n        names.arg = regressor_names,\n        main = \"Significance of Regressors on Car Prices\",\n        xlab = \"Regressors\",\n        ylab = \"-log10(p-value)\",\n        col = \"lightblue\",\n        ylim = c(0, max(-log10(p_values)) * 1.2),\n        las = 2, # Rotate x-axis labels vertically\n        cex.names = 0.7)\n\n# Add a horizontal dashed line at significance level (for example, p = 0.05)\nabline(h = -log10(0.05), lty = 2, col = \"red\")\n\n\n\n\n\n\n\n# Determine significance levels based on p-values\nsignificance_levels &lt;- ifelse(p_values &lt;= 0.001, \"***\",\n                              ifelse(p_values &lt;= 0.01, \"**\",\n                                     ifelse(p_values &lt;= 0.05, \"*\", \"\")))\n\n# Filter significant regressors whose p-values are less than 0.05\nsignificant_regressors &lt;- regressor_names[p_values &lt; 0.05]\n\n# Create a table of significant regressors along with their significance levels\n  significant_table &lt;- data.frame(\n  Regressors = significant_regressors,\n  P_Values = p_values[p_values &lt; 0.05],\n  Significance = significance_levels[p_values &lt; 0.05]\n)\nsignificant_table\n\n\n\n  \n\n\n# Include only significant regressors\nmodel_imp &lt;- lm(price ~ enginelocation + enginesize + stroke + carwidth + peakrpm, data = data)\nsummary(model_imp)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ enginelocation + enginesize + stroke + carwidth + \n#&gt;     peakrpm, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -9332  -1532   -235   1132  15349 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        -6.886e+04  1.055e+04  -6.529 6.91e-10 ***\n#&gt; enginelocationrear  1.100e+04  1.987e+03   5.536 1.11e-07 ***\n#&gt; enginesize          1.355e+02  8.905e+00  15.211  &lt; 2e-16 ***\n#&gt; stroke             -2.768e+03  7.194e+02  -3.847 0.000167 ***\n#&gt; carwidth            9.568e+02  1.671e+02   5.725 4.41e-08 ***\n#&gt; peakrpm             2.023e+00  5.634e-01   3.591 0.000427 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2948 on 175 degrees of freedom\n#&gt; Multiple R-squared:  0.8702, Adjusted R-squared:  0.8665 \n#&gt; F-statistic: 234.7 on 5 and 175 DF,  p-value: &lt; 2.2e-16\n\n# Extract the coefficients of significant regressors from the regression model summary\nsignificant_regressors &lt;- c(\"enginelocationrear\", \"enginesize\", \"stroke\", \"carwidth\", \"peakrpm\")\n\n# Extract the coefficients and variable names\ncoefficients &lt;- coef(model)[significant_regressors]\n\n# Create a bar plot to visualize the impact of significant regressors on price\nbarplot(coefficients, \n        main = \"Impact of Significant Regressors on Car Prices\",\n        xlab = \"Regressors\",\n        ylab = \"Coefficient Estimate\",\n        col = \"skyblue\",\n        ylim = c(min(coefficients) * 1.2, max(coefficients) * 1.2),\n        las = 2, # Rotate x-axis labels vertically\n        beside = TRUE)\n\n# Add labels for the variables\ntext(1:length(coefficients), coefficients, labels = round(coefficients, digits = 2), pos = 3, cex = 0.8)"
  },
  {
    "objectID": "content/01_journal/03_regression.html#assignment-3.4",
    "href": "content/01_journal/03_regression.html#assignment-3.4",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Now for this part enginesize is selected based on its p-value (4.887489e-06) and significance and also a positive correletion based on the plot and positive estimate of 125.72 show that as enginesize increases, price also increases. The data type of enginesize is  double and it is statistically significant since its p-value is less than alpha= 0.05.\n\n# Plot relationship between enginesize and price\nggplot(data, aes(x = enginesize, y = price)) +\n  geom_point(alpha = 0.8)\n\n\n\n\n\n\n\n\n\n# Step 5: Add a new variable 'seat_heating' and run regression\ndata_with_seat_heating &lt;- data %&gt;% mutate(seat_heating = TRUE)\nmodel_with_seat_heating &lt;- lm(price ~ ., data = data_with_seat_heating)\nsummary(model_with_seat_heating) # Check regression summary for the coefficient of seat_heating\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = data_with_seat_heating)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; seat_heatingTRUE             NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nAdding a variable seat_heating to the data and assigning a value TRUE for all observations gave a NA for Estimate coefficient in the new regression model. This new variable has no affect on the price since it is same for all observations."
  },
  {
    "objectID": "content/01_journal/10_rdd.html#assignment-10.1",
    "href": "content/01_journal/10_rdd.html#assignment-10.1",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "#load libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rddensity)\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ lubridate 1.9.3     ✔ tibble    3.2.1\n#&gt; ✔ purrr     1.0.2     ✔ tidyr     1.3.0\n#&gt; ✔ readr     2.1.4     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read data for coupon.rds\ndf &lt;- readRDS(\"data/coupon.rds\")\n\n# Define cut-off\nc0 &lt;- 60\nbw&lt;- c0+c(-2.5,2.5)\ndf\n\n\n\n  \n\n\n# [2.2] Random assignment test for half bandwidth ----\nggplot(df, aes(x = days_since_last, fill = coupon)) +\n  geom_histogram(binwidth = 4, color = \"white\", alpha = 0.6) +\n  geom_vline(xintercept = c0 - 30, color = \"red\", linetype = \"dashed\") + # Half bandwidth\n  scale_fill_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Number of customers\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n# [3.2] LATE for half bandwidth ----\ndf_below_half &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_above_half &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\nmodel_below_half &lt;- lm(purchase_after ~ days_since_last, data = df_below_half)\nmodel_above_half &lt;- lm(purchase_after ~ days_since_last, data = df_above_half)\n\npred_below_half &lt;- predict(model_below_half, tibble(days_since_last = c0))\npred_above_half &lt;- predict(model_above_half, tibble(days_since_last = c0))\n\nlate_half &lt;- pred_above_half - pred_below_half\nsprintf(\"LATE with half bandwidth: %.2f\", late_half)\n\n#&gt; [1] \"LATE with half bandwidth: 7.36\"\n\n# [4.1] Estimation with half bandwidth ----\nlm_bw_half &lt;- lm(purchase_after ~ days_since_last, data = df)\nsummary(lm_bw_half)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -14.244  -3.620  -0.558   2.868  34.353 \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     12.353269   0.117865  104.81   &lt;2e-16 ***\n#&gt; days_since_last  0.053425   0.002281   23.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.331 on 4998 degrees of freedom\n#&gt; Multiple R-squared:  0.09889,    Adjusted R-squared:  0.09871 \n#&gt; F-statistic: 548.5 on 1 and 4998 DF,  p-value: &lt; 2.2e-16\n\nggplot(df, aes(x = days_since_last, y = purchase_after)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_vline(xintercept = c0 - 30, linetype = \"dashed\", color = \"red\") + # Half bandwidth\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after campaign\") +\n  ggtitle(\"Regression lines for half bandwidth\")\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#assignment-10.2",
    "href": "content/01_journal/10_rdd.html#assignment-10.2",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "# [2.2] Random assignment test for double bandwidth ----\nggplot(df, aes(x = days_since_last, fill = coupon)) +\n  geom_histogram(binwidth = 4, color = \"white\", alpha = 0.6) +\n  geom_vline(xintercept = c0 + 30, color = \"red\", linetype = \"dashed\") + # Double bandwidth\n  scale_fill_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Number of customers\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n# [3.2] LATE for double bandwidth ----\nbw &lt;- c0+c(-10,10)\ndf_below_double &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_above_double &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\nmodel_below_double &lt;- lm(purchase_after ~ days_since_last, data = df_below_double)\nmodel_above_double &lt;- lm(purchase_after ~ days_since_last, data = df_above_double)\n\npred_below_double &lt;- predict(model_below_double, tibble(days_since_last = c0))\npred_above_double &lt;- predict(model_above_double, tibble(days_since_last = c0))\n\nlate_double &lt;- mean(pred_above_double) - mean(pred_below_double)\nsprintf(\"LATE with double bandwidth: %.2f\", late_double)\n\n#&gt; [1] \"LATE with double bandwidth: 9.51\"\n\n# [4.1] Estimation with double bandwidth ----\nlm_bw_double &lt;- lm(purchase_after ~ days_since_last, data = df)\nsummary(lm_bw_double)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -14.244  -3.620  -0.558   2.868  34.353 \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     12.353269   0.117865  104.81   &lt;2e-16 ***\n#&gt; days_since_last  0.053425   0.002281   23.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.331 on 4998 degrees of freedom\n#&gt; Multiple R-squared:  0.09889,    Adjusted R-squared:  0.09871 \n#&gt; F-statistic: 548.5 on 1 and 4998 DF,  p-value: &lt; 2.2e-16\n\nggplot(df, aes(x = days_since_last, y = purchase_after)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_vline(xintercept = c0 + 30, linetype = \"dashed\", color = \"red\") + # Double bandwidth\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after campaign\") +\n  ggtitle(\"Regression lines for double bandwidth\")\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAfter running the analysis it was noticed that making the bandwidth half resulted in LATE value of 7.53. Where as, doubling the bandwidth resulted in a higher LATE value of 9.51."
  },
  {
    "objectID": "content/01_journal/10_rdd.html#assignment-10.3",
    "href": "content/01_journal/10_rdd.html#assignment-10.3",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "# Read data for shipping.rds\nshipping_df &lt;- readRDS(\"data/shipping.rds\")\nshipping_df\n\n\n\n  \n\n\n# Plot to evaluate 'purchase_amount' as a running variable with a cutoff at 30€\nggplot(shipping_df, aes(x = purchase_amount, fill = purchase_amount &gt; 30)) +\n  geom_histogram(binwidth = 5, color = \"white\", alpha = 0.6) +\n  scale_fill_manual(values = c(\"blue\", \"red\"), labels = c(\"&lt;= 30€\", \"&gt; 30€\")) +\n  geom_vline(xintercept = 30, linetype = \"dashed\", color = \"black\") +\n  xlab(\"Purchase Amount (€)\") +\n  ylab(\"Number of Purchases\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nFor RDD to deliver valid results we have to make sure there is no non-random heaping at the cut-off, i.e. no manipulation because for example the effect is known and units attempt to move to one side of the cut-off. We can plot the distribution around the cut-off to check for violations of the continuity assumption.\nWe can see that there is indeed decline or incline at the cut-off and therefore we cannot assume that the continuity assumption holds.\nTo check the continuity assumption more thoroughly, we can also use functions of the rddensity package\n\n# Density test\n# Check for continuous density along running variable. Manipulations could \n# lead to running variable being \"crowded\" right after cutoff.\nrddd &lt;- rddensity(shipping_df$purchase_amount, c = 30)\nsummary(rddd)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0\n\n\n#&gt; Warning in summary.CJMrddensity(rddd): There are repeated observations. Point\n#&gt; estimates and standard errors have been adjusted. Use option massPoints=FALSE\n#&gt; to suppress this feature.\n\n\n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\n# Visually check continuity at running variable\nrdd_plot &lt;- rdplotdensity(rddd, shipping_df$purchase_amount, plotN = 100)\n\n\n\n\n\n\n\n\nThe plot confirms our assumption. we can see that the confidence intervals do not overlap. Since they do not overlap, we would have to suspect some kind of manipulation around the cut-off and could not use RDD to obtain valid results."
  }
]